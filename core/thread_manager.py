import asyncio
import json
from typing import Any, AsyncGenerator, Dict, List, Literal, Optional, Type, Union, cast

from loguru import logger

from core.error_processor import ErrorProcessor
from core.response_processor import ProcessorConfig, ResponseProcessor
from core.services.db import get_db
from core.services.llm import LLMError, make_llm_api_call
from core.tool import Tool
from models.message import Message, MessageModel
from models.thread import Thread

ToolChoice = Literal["auto", "required", "none"]


class ThreadManager:
    """ç®¡ç†å¯¹è¯çº¿ç¨‹ï¼Œé›†æˆLLMæ¨¡å‹å’Œå·¥å…·æ‰§è¡Œã€‚"""

    def __init__(
        self,
        agent_config: Optional[dict] = None,
    ):
        # self.tool_registry = ToolRegistry()

        self.agent_config = agent_config
        self.response_processor = ResponseProcessor(
            add_message_callback=self.add_message,
            agent_config=self.agent_config,
        )

    def add_tool(
        self,
        tool_class: Type[Tool],
        function_names: Optional[List[str]] = None,
        **kwargs,
    ):
        """å‘ThreadManageræ·»åŠ å·¥å…·ã€‚"""
        # self.tool_registry.register_tool(tool_class, function_names, **kwargs)
        pass

    async def create_thread(
        self,
        account_id: Optional[str] = None,
        project_id: Optional[str] = None,
        is_public: bool = False,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """åœ¨æ•°æ®åº“ä¸­åˆ›å»ºæ–°çº¿ç¨‹ã€‚"""
        # logger.debug(f"åˆ›å»ºæ–°çº¿ç¨‹ (account_id: {account_id}, project_id: {project_id})")

        thread_data = {"is_public": is_public, "metadata": metadata or {}}
        if account_id:
            thread_data["account_id"] = account_id
        if project_id:
            thread_data["project_id"] = project_id

        try:
            with get_db() as db:
                thread = Thread(**thread_data)
                db.add(thread)
                db.commit()
                db.refresh(thread)

            if thread:
                thread_id = thread.thread_id
                logger.info(f"æˆåŠŸåˆ›å»ºçº¿ç¨‹: {thread_id}")
                return thread_id
            else:
                raise Exception("åˆ›å»ºçº¿ç¨‹å¤±è´¥: æœªè¿”å›thread_id")
        except Exception as e:
            logger.error(f"åˆ›å»ºçº¿ç¨‹å¤±è´¥: {str(e)}", exc_info=True)
            raise Exception(f"çº¿ç¨‹åˆ›å»ºå¤±è´¥: {str(e)}")

    async def add_message(
        self,
        thread_id: str,
        type: str,
        content: Union[Dict[str, Any], List[Any], str],
        is_llm_message: bool = False,
        metadata: Optional[Dict[str, Any]] = None,
        agent_id: Optional[str] = None,
        agent_version_id: Optional[str] = None,
    ):
        """å‘çº¿ç¨‹ä¸­æ·»åŠ æ¶ˆæ¯åˆ°æ•°æ®åº“ã€‚"""
        # logger.debug(f"å‘çº¿ç¨‹ {thread_id} æ·»åŠ ç±»å‹ä¸º '{type}' çš„æ¶ˆæ¯")

        data_to_insert = {
            "thread_id": thread_id,
            "type": type,
            "content": content,
            "is_llm_message": is_llm_message,
            "metadata": metadata or {},
        }

        if agent_id:
            data_to_insert["agent_id"] = agent_id
        if agent_version_id:
            data_to_insert["agent_version_id"] = agent_version_id

        try:
            with get_db() as db:
                message = Message(**data_to_insert)
                db.add(message)
                db.commit()
                db.refresh(message)

            if message:
                saved_message = MessageModel.model_validate(message)

                return saved_message
            else:
                logger.error(f"çº¿ç¨‹ {thread_id} çš„æ’å…¥æ“ä½œå¤±è´¥")
                return None
        except Exception as e:
            logger.error(f"å‘çº¿ç¨‹ {thread_id} æ·»åŠ æ¶ˆæ¯å¤±è´¥: {str(e)}", exc_info=True)
            raise

    async def get_llm_messages(self, thread_id: str) -> List[Dict[str, Any]]:
        """è·å–çº¿ç¨‹çš„æ‰€æœ‰æ¶ˆæ¯ã€‚"""
        logger.debug(f"è·å–çº¿ç¨‹ {thread_id} çš„æ¶ˆæ¯")

        try:
            all_messages = []
            batch_size = 1000
            offset = 0

            while True:
                with get_db() as db:
                    result = (
                        db.query(
                            Message.message_id,
                            Message.type,
                            Message.content,
                            Message.metadata,
                        )
                        .filter(Message.thread_id == thread_id)
                        .filter(Message.is_llm_message == True)
                        .order_by(Message.created_at)
                        .offset(offset)
                        .limit(batch_size)
                        .all()
                    )

                if not result:
                    break

                all_messages.extend(result)
                if len(result) < batch_size:
                    break
                offset += batch_size

            if not all_messages:
                return []

            messages = []
            for item in all_messages:
                # æ£€æŸ¥æ­¤æ¶ˆæ¯åœ¨å…ƒæ•°æ®ä¸­æ˜¯å¦æœ‰å‹ç¼©ç‰ˆæœ¬
                content = item["content"]
                metadata = item.get("metadata", {})
                is_compressed = False

                # å¦‚æœå·²å‹ç¼©ï¼Œå¯¹LLMä½¿ç”¨compressed_contentè€Œä¸æ˜¯å®Œæ•´å†…å®¹
                if isinstance(metadata, dict) and metadata.get("compressed"):
                    compressed_content = metadata.get("compressed_content")
                    if compressed_content:
                        content = compressed_content
                        is_compressed = True
                        # logger.debug(f"å¯¹æ¶ˆæ¯ {item['message_id']} ä½¿ç”¨å‹ç¼©å†…å®¹")

                # è§£æå†…å®¹å¹¶æ·»åŠ message_id
                if isinstance(content, str):
                    try:
                        parsed_item = json.loads(content)
                        parsed_item["message_id"] = item["message_id"]
                        messages.append(parsed_item)
                    except json.JSONDecodeError:
                        # å¦‚æœå·²å‹ç¼©ï¼Œå†…å®¹æ˜¯çº¯å­—ç¬¦ä¸²(ä¸æ˜¯JSON) - è¿™æ˜¯é¢„æœŸçš„
                        if is_compressed:
                            messages.append(
                                {
                                    "role": "user",
                                    "content": content,
                                    "message_id": item["message_id"],
                                }
                            )
                        else:
                            logger.error(f"è§£ææ¶ˆæ¯å¤±è´¥: {content[:100]}")
                else:
                    content["message_id"] = item["message_id"]
                    messages.append(content)

            return messages

        except Exception as e:
            logger.error(
                f"è·å–çº¿ç¨‹ {thread_id} çš„æ¶ˆæ¯å¤±è´¥: {str(e)}",
                exc_info=True,
            )
            return []

    async def run_thread(
        self,
        thread_id: str,
        system_prompt: Dict[str, Any],
        stream: bool = True,
        temporary_message: Optional[Dict[str, Any]] = None,
        llm_model: str = "gpt-5",
        llm_temperature: float = 0,
        llm_max_tokens: Optional[int] = None,
        processor_config: Optional[ProcessorConfig] = None,
        tool_choice: ToolChoice = "auto",
        native_max_auto_continues: int = 25,
        max_xml_tool_calls: int = 0,
        latest_user_message_content: Optional[str] = None,
        cancellation_event: Optional[asyncio.Event] = None,
    ) -> Union[Dict[str, Any], AsyncGenerator]:
        """è¿è¡Œå¯¹è¯çº¿ç¨‹ï¼Œé›†æˆLLMå’Œå·¥å…·æ‰§è¡Œã€‚"""
        logger.debug(f"ğŸš€ å¼€å§‹æ‰§è¡Œçº¿ç¨‹ {thread_id}ï¼Œä½¿ç”¨æ¨¡å‹ {llm_model}")

        # ç¡®ä¿æˆ‘ä»¬æœ‰æœ‰æ•ˆçš„ProcessorConfigå¯¹è±¡
        if processor_config is None:
            config = ProcessorConfig()
        elif isinstance(processor_config, ProcessorConfig):
            config = processor_config
        else:
            logger.error(
                f"æ— æ•ˆçš„processor_configç±»å‹: {type(processor_config)}ï¼Œåˆ›å»ºé»˜è®¤å€¼"
            )
            config = ProcessorConfig()

        auto_continue_state = {
            "count": 0,
            "active": True,
            "continuous_state": {"accumulated_content": "", "thread_run_id": None},
        }

        # å¦‚æœç¦ç”¨è‡ªåŠ¨ç»§ç»­ï¼Œåˆ™å•æ¬¡æ‰§è¡Œ
        if native_max_auto_continues == 0:
            result = await self._execute_run(
                thread_id,
                system_prompt,
                llm_model,
                llm_temperature,
                llm_max_tokens,
                tool_choice,
                config,
                stream,
                auto_continue_state,
                temporary_message,
                latest_user_message_content,
                cancellation_event,
            )

            # å¦‚æœç»“æœæ˜¯é”™è¯¯å­—å…¸ï¼Œå°†å…¶è½¬æ¢ä¸ºç”Ÿæˆå™¨å¹¶äº§å‡ºé”™è¯¯
            if isinstance(result, dict) and result.get("status") == "error":
                return self._create_single_error_generator(result)

            return result

        # è‡ªåŠ¨ç»§ç»­æ‰§è¡Œ
        return self._auto_continue_generator(
            thread_id,
            system_prompt,
            llm_model,
            llm_temperature,
            llm_max_tokens,
            tool_choice,
            config,
            stream,
            auto_continue_state,
            temporary_message,
            native_max_auto_continues,
            latest_user_message_content,
            cancellation_event,
        )

    async def _execute_run(
        self,
        thread_id: str,
        system_prompt: Dict[str, Any],
        llm_model: str,
        llm_temperature: float,
        llm_max_tokens: Optional[int],
        tool_choice: ToolChoice,
        config: ProcessorConfig,
        stream: bool,
        auto_continue_state: Dict[str, Any],
        temporary_message: Optional[Dict[str, Any]] = None,
        latest_user_message_content: Optional[str] = None,
        cancellation_event: Optional[asyncio.Event] = None,
    ) -> Union[Dict[str, Any], AsyncGenerator]:
        """æ‰§è¡Œå•æ¬¡LLMè¿è¡Œã€‚"""

        # å…³é”®: ç¡®ä¿configå§‹ç»ˆæ˜¯ProcessorConfigå¯¹è±¡
        if not isinstance(config, ProcessorConfig):
            logger.error(
                f"é”™è¯¯: configæ˜¯{type(config)}ï¼ŒæœŸæœ›ProcessorConfigã€‚å€¼: {config}"
            )
            config = ProcessorConfig()  # åˆ›å»ºæ–°å®ä¾‹ä½œä¸ºåå¤‡

        try:
            estimated_total_tokens = None  # å°†ä¼ é€’ç»™å“åº”å¤„ç†å™¨ä»¥é¿å…é‡æ–°è®¡ç®—

            # å…³é”®: é¦–å…ˆæ£€æŸ¥è¿™æ˜¯å¦æ˜¯è‡ªåŠ¨ç»§ç»­è¿­ä»£(åœ¨ä»»ä½•tokenè®¡æ•°ä¹‹å‰)
            is_auto_continue = auto_continue_state.get("count", 0) > 0

            # å§‹ç»ˆè·å–æ¶ˆæ¯(éœ€è¦ç”¨äºLLMè°ƒç”¨)
            # å¿«é€Ÿè·¯å¾„åªæ˜¯è·³è¿‡å‹ç¼©ï¼Œè€Œä¸æ˜¯è·å–ï¼
            messages = await self.get_llm_messages(thread_id)

            # å¤„ç†è‡ªåŠ¨ç»§ç»­ä¸Šä¸‹æ–‡
            if auto_continue_state["count"] > 0 and auto_continue_state[
                "continuous_state"
            ].get("accumulated_content"):
                partial_content = auto_continue_state["continuous_state"][
                    "accumulated_content"
                ]
                messages.append({"role": "assistant", "content": partial_content})

            # è·å–LLMè°ƒç”¨çš„å·¥å…·æ¨¡å¼(åœ¨å‹ç¼©ä¹‹å)
            openapi_tool_schemas = None

            prepared_messages = messages

            # æ³¨æ„: æˆ‘ä»¬ä¸åœ¨æ­¤å¤„è®°å½•tokenè®¡æ•°ï¼Œå› ä¸ºç¼“å­˜å—ç»™å‡ºä¸å‡†ç¡®çš„è®¡æ•°
            # LLMçš„usage.prompt_tokens(åœ¨è°ƒç”¨åæŠ¥å‘Š)æ˜¯å‡†ç¡®çš„çœŸç›¸æ¥æº
            logger.info(f"ğŸ“¤ å‘LLMå‘é€ {len(prepared_messages)} æ¡å‡†å¤‡å¥½çš„æ¶ˆæ¯")

            # è°ƒç”¨LLM
            try:
                llm_response = await make_llm_api_call(
                    prepared_messages,
                    llm_model,
                    temperature=llm_temperature,
                    max_tokens=llm_max_tokens,
                    tools=openapi_tool_schemas,
                    stream=stream,
                )
            except LLMError as e:
                return {"type": "status", "status": "error", "message": str(e)}

            # æ£€æŸ¥é”™è¯¯å“åº”
            if isinstance(llm_response, dict) and llm_response.get("status") == "error":
                return llm_response

            if stream and hasattr(llm_response, "__aiter__"):
                return self.response_processor.process_streaming_response(
                    cast(AsyncGenerator, llm_response),
                    thread_id,
                    prepared_messages,
                    llm_model,
                    config,
                    True,
                    auto_continue_state["count"],
                    auto_continue_state["continuous_state"],
                    estimated_total_tokens,
                    cancellation_event,
                )
            else:
                return self.response_processor.process_non_streaming_response(
                    llm_response,
                    thread_id,
                    prepared_messages,
                    llm_model,
                    config,
                    estimated_total_tokens,
                )

        except Exception as e:
            processed_error = ErrorProcessor.process_system_error(
                e, context={"thread_id": thread_id}
            )
            ErrorProcessor.log_error(processed_error)
            return processed_error.to_stream_dict()

    async def _auto_continue_generator(
        self,
        thread_id: str,
        system_prompt: Dict[str, Any],
        llm_model: str,
        llm_temperature: float,
        llm_max_tokens: Optional[int],
        tool_choice: ToolChoice,
        config: ProcessorConfig,
        stream: bool,
        auto_continue_state: Dict[str, Any],
        temporary_message: Optional[Dict[str, Any]],
        native_max_auto_continues: int,
        latest_user_message_content: Optional[str] = None,
        cancellation_event: Optional[asyncio.Event] = None,
    ) -> AsyncGenerator:
        """å¤„ç†è‡ªåŠ¨ç»§ç»­é€»è¾‘çš„ç”Ÿæˆå™¨ã€‚"""
        logger.debug(f"å¯åŠ¨è‡ªåŠ¨ç»§ç»­ç”Ÿæˆå™¨ï¼Œæœ€å¤§æ¬¡æ•°: {native_max_auto_continues}")
        # logger.debug(f"è‡ªåŠ¨ç»§ç»­ç”Ÿæˆå™¨ä¸­çš„Configç±»å‹: {type(config)}")

        # ç¡®ä¿configæ˜¯æœ‰æ•ˆçš„ProcessorConfig
        if not isinstance(config, ProcessorConfig):
            logger.error(f"è‡ªåŠ¨ç»§ç»­ä¸­æ— æ•ˆçš„configç±»å‹: {type(config)}ï¼Œåˆ›å»ºæ–°çš„")
            config = ProcessorConfig()

        while (
            auto_continue_state["active"]
            and auto_continue_state["count"] < native_max_auto_continues
        ):
            auto_continue_state["active"] = False  # é‡ç½®æœ¬æ¬¡è¿­ä»£

            try:
                # ç»§ç»­å‰æ£€æŸ¥å–æ¶ˆä¿¡å·
                if cancellation_event and cancellation_event.is_set():
                    logger.info(f"çº¿ç¨‹ {thread_id} çš„è‡ªåŠ¨ç»§ç»­ç”Ÿæˆå™¨æ”¶åˆ°å–æ¶ˆä¿¡å·")
                    break

                response_gen = await self._execute_run(
                    thread_id,
                    system_prompt,
                    llm_model,
                    llm_temperature,
                    llm_max_tokens,
                    tool_choice,
                    config,
                    stream,
                    auto_continue_state,
                    temporary_message if auto_continue_state["count"] == 0 else None,
                    latest_user_message_content
                    if auto_continue_state["count"] == 0
                    else None,
                    cancellation_event,
                )

                # å¤„ç†é”™è¯¯å“åº”
                if (
                    isinstance(response_gen, dict)
                    and response_gen.get("status") == "error"
                ):
                    yield response_gen
                    break

                # å¤„ç†æµå¼å“åº”
                if hasattr(response_gen, "__aiter__"):
                    async for chunk in cast(AsyncGenerator, response_gen):
                        # æ£€æŸ¥å–æ¶ˆä¿¡å·
                        if cancellation_event and cancellation_event.is_set():
                            logger.info(
                                f"å¤„ç†çº¿ç¨‹ {thread_id} è‡ªåŠ¨ç»§ç»­æµæ—¶æ”¶åˆ°å–æ¶ˆä¿¡å·"
                            )
                            break

                        # æ£€æŸ¥è‡ªåŠ¨ç»§ç»­è§¦å‘å™¨
                        should_continue = self._check_auto_continue_trigger(
                            chunk, auto_continue_state, native_max_auto_continues
                        )

                        # è·³è¿‡è§¦å‘è‡ªåŠ¨ç»§ç»­çš„å®Œæˆå—(ä½†ä¸æ˜¯å·¥å…·æ‰§è¡Œï¼Œå‰ç«¯éœ€è¦é‚£äº›)
                        if should_continue:
                            if chunk.get("type") == "status":
                                try:
                                    content = json.loads(chunk.get("content", "{}"))
                                    # ä»…è·³è¿‡é•¿åº¦é™åˆ¶å®ŒæˆçŠ¶æ€(å‰ç«¯éœ€è¦å·¥å…·æ‰§è¡Œå®Œæˆ)
                                    if content.get("finish_reason") == "length":
                                        continue
                                except (json.JSONDecodeError, TypeError):
                                    pass

                        yield chunk
                else:
                    yield response_gen

                if not auto_continue_state["active"]:
                    break

            except Exception as e:
                processed_error = ErrorProcessor.process_system_error(
                    e, context={"thread_id": thread_id}
                )
                ErrorProcessor.log_error(processed_error)
                yield processed_error.to_stream_dict()
                return

        # å¤„ç†è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        if (
            auto_continue_state["active"]
            and auto_continue_state["count"] >= native_max_auto_continues
        ):
            logger.warning(f"è¾¾åˆ°æœ€å¤§è‡ªåŠ¨ç»§ç»­é™åˆ¶ ({native_max_auto_continues})")
            yield {
                "type": "content",
                "content": f"\n[Agentè¾¾åˆ°æœ€å¤§è‡ªåŠ¨ç»§ç»­é™åˆ¶ {native_max_auto_continues}]",
            }

    def _check_auto_continue_trigger(
        self,
        chunk: Dict[str, Any],
        auto_continue_state: Dict[str, Any],
        native_max_auto_continues: int,
    ) -> bool:
        """æ£€æŸ¥å“åº”å—æ˜¯å¦åº”è¯¥è§¦å‘è‡ªåŠ¨ç»§ç»­ã€‚"""
        if chunk.get("type") == "status":
            try:
                content = (
                    json.loads(chunk.get("content", "{}"))
                    if isinstance(chunk.get("content"), str)
                    else chunk.get("content", {})
                )
                finish_reason = content.get("finish_reason")
                tools_executed = content.get("tools_executed", False)

                # ä¸ºä»¥ä¸‹æƒ…å†µè§¦å‘è‡ªåŠ¨ç»§ç»­: åŸç”Ÿå·¥å…·è°ƒç”¨ã€é•¿åº¦é™åˆ¶æˆ–XMLå·¥å…·å·²æ‰§è¡Œ
                if finish_reason == "tool_calls" or tools_executed:
                    if native_max_auto_continues > 0:
                        logger.debug(
                            f"å› å·¥å…·æ‰§è¡Œè‡ªåŠ¨ç»§ç»­ ({auto_continue_state['count'] + 1}/{native_max_auto_continues})"
                        )
                        auto_continue_state["active"] = True
                        auto_continue_state["count"] += 1
                        return True
                elif finish_reason == "length":
                    logger.debug(
                        f"å› é•¿åº¦é™åˆ¶è‡ªåŠ¨ç»§ç»­ ({auto_continue_state['count'] + 1}/{native_max_auto_continues})"
                    )
                    auto_continue_state["active"] = True
                    auto_continue_state["count"] += 1
                    return True
                elif finish_reason == "xml_tool_limit_reached":
                    logger.debug("å› XMLå·¥å…·é™åˆ¶åœæ­¢è‡ªåŠ¨ç»§ç»­")
                    auto_continue_state["active"] = False
            except (json.JSONDecodeError, TypeError):
                pass

        return False

    async def _create_single_error_generator(self, error_dict: Dict[str, Any]):
        """åˆ›å»ºäº§å‡ºå•ä¸ªé”™è¯¯æ¶ˆæ¯çš„å¼‚æ­¥ç”Ÿæˆå™¨ã€‚"""
        yield error_dict
