import asyncio
import json
from collections.abc import AsyncGenerator
from datetime import datetime
from typing import Any, Literal, Optional, Union, cast

from loguru import logger
from sqlalchemy import select

from core.error_processor import ErrorProcessor
from core.response_processor import ProcessorConfig, ResponseProcessor
from core.services.db import get_db
from core.services.llm import LLMError, make_llm_api_call
from core.tool import Tool
from models.message import Message, Messages
from models.thread import Thread, Threads

ToolChoice = Literal["auto", "required", "none"]


class ThreadManager:
    """ç®¡ç†å¯¹è¯çº¿ç¨‹ï¼Œé›†æˆLLMæ¨¡å‹å’Œå·¥å…·æ‰§è¡Œã€‚"""

    def __init__(
        self,
        agent_config: Optional[dict] = None,
    ):
        # self.tool_registry = ToolRegistry()

        self.agent_config = agent_config
        self.response_processor = ResponseProcessor(
            add_message_callback=self.add_message,
            agent_config=self.agent_config,
        )

    def add_tool(
        self,
        tool_class: type[Tool],
        function_names: Optional[list[str]] = None,
        **kwargs,
    ):
        """å‘ThreadManageræ·»åŠ å·¥å…·ã€‚"""
        # self.tool_registry.register_tool(tool_class, function_names, **kwargs)
        pass

    async def create_thread(
        self,
        account_id: Optional[str] = None,
        project_id: Optional[str] = None,
        is_public: bool = False,
        metadata: Optional[dict[str, Any]] = None,
    ) -> str:
        """åœ¨æ•°æ®åº“ä¸­åˆ›å»ºæ–°çº¿ç¨‹ã€‚"""
        # logger.debug(f"åˆ›å»ºæ–°çº¿ç¨‹ (account_id: {account_id}, project_id: {project_id})")

        thread_data = {"is_public": is_public, "metadata": metadata or {}}
        if account_id:
            thread_data["account_id"] = account_id
        if project_id:
            thread_data["project_id"] = project_id

        thread = Threads.insert(Thread(**thread_data))
        thread_id = thread.thread_id
        logger.info("æˆåŠŸåˆ›å»ºçº¿ç¨‹: {}", thread_id)
        return thread_id

    async def add_message(
        self,
        thread_id: str,
        type: str,
        content: Union[dict[str, Any], list[Any], str],
        is_llm_message: bool = False,
        metadata: Optional[dict[str, Any]] = None,
        agent_id: Optional[str] = None,
        agent_version_id: Optional[str] = None,
    ):
        """å‘çº¿ç¨‹ä¸­æ·»åŠ æ¶ˆæ¯åˆ°æ•°æ®åº“ã€‚"""
        # logger.debug(f"å‘çº¿ç¨‹ {thread_id} æ·»åŠ ç±»å‹ä¸º '{type}' çš„æ¶ˆæ¯")

        current_time = datetime.now()
        data_to_insert = {
            "thread_id": thread_id,
            "type": type,
            "content": content,
            "is_llm_message": is_llm_message,
            "meta": metadata or {},
            "created_at": current_time,
            "updated_at": current_time,
        }

        if agent_id:
            data_to_insert["agent_id"] = agent_id
        if agent_version_id:
            data_to_insert["agent_version_id"] = agent_version_id

        message = Message(**data_to_insert)
        saved_message = Messages.insert(message)
        return saved_message.model_dump(mode="json")

    async def get_llm_messages(self, thread_id: str) -> list[dict[str, Any]]:
        """è·å–çº¿ç¨‹çš„æ‰€æœ‰æ¶ˆæ¯ã€‚"""
        logger.debug(f"è·å–çº¿ç¨‹ {thread_id} çš„æ¶ˆæ¯")

        try:
            all_messages = []
            batch_size = 1000
            offset = 0

            while True:
                with get_db() as db:
                    result = (
                        db.execute(
                            select(
                                Message.message_id,
                                Message.type,
                                Message.content,
                                Message.meta,
                            )
                            .filter(Message.thread_id == thread_id)
                            .filter(Message.is_llm_message)
                            .order_by(Message.created_at)
                            .offset(offset)
                            .limit(batch_size)
                        )
                        .mappings()
                        .all()
                    )

                if not result:
                    break

                all_messages.extend(result)
                if len(result) < batch_size:
                    break
                offset += batch_size

            if not all_messages:
                return []

            messages = []
            for item in all_messages:
                # æ£€æŸ¥æ­¤æ¶ˆæ¯åœ¨å…ƒæ•°æ®ä¸­æ˜¯å¦æœ‰å‹ç¼©ç‰ˆæœ¬
                content = item["content"]
                metadata = item.get("meta", {})

                # è§£æå†…å®¹å¹¶æ·»åŠ message_id
                if isinstance(content, str):
                    try:
                        parsed_item = json.loads(content)
                        parsed_item["message_id"] = item["message_id"]
                        messages.append(parsed_item)
                    except json.JSONDecodeError:
                        logger.error(f"è§£ææ¶ˆæ¯å¤±è´¥: {content[:100]}")
                else:
                    content["message_id"] = str(item["message_id"])
                    messages.append(content)

            return messages

        except Exception as e:
            logger.exception(f"è·å–çº¿ç¨‹ {thread_id} çš„æ¶ˆæ¯å¤±è´¥")
            return []

    async def run_thread(
        self,
        thread_id: str,
        system_prompt: dict[str, Any],
        stream: bool = True,
        temporary_message: Optional[dict[str, Any]] = None,
        llm_model: str = "glm-4.6",
        processor_config: Optional[ProcessorConfig] = None,
        tool_choice: ToolChoice = "auto",
        latest_user_message_content: Optional[str] = None,
        cancellation_event: Optional[asyncio.Event] = None,
    ) -> Union[dict[str, Any], AsyncGenerator]:
        """è¿è¡Œå¯¹è¯çº¿ç¨‹ï¼Œé›†æˆLLMå’Œå·¥å…·æ‰§è¡Œã€‚"""
        logger.debug(f"ğŸš€ å¼€å§‹æ‰§è¡Œçº¿ç¨‹ {thread_id}ï¼Œä½¿ç”¨æ¨¡å‹ {llm_model}")

        # ç¡®ä¿ProcessorConfigå¯¹è±¡æœ‰æ•ˆ
        if processor_config is None:
            config = ProcessorConfig()
        elif isinstance(processor_config, ProcessorConfig):
            config = processor_config
        else:
            logger.error(f"æ— æ•ˆçš„processor_configç±»å‹: {type(processor_config)}ï¼Œåˆ›å»ºé»˜è®¤å€¼")
            config = ProcessorConfig()

        result = await self._execute_run(
            thread_id,
            system_prompt,
            llm_model,
            tool_choice,
            config,
            stream,
            # auto_continue_state,
            temporary_message,
            latest_user_message_content,
            cancellation_event,
        )

        # å¦‚æœç»“æœæ˜¯é”™è¯¯å­—å…¸ï¼Œå°†å…¶è½¬æ¢ä¸ºç”Ÿæˆå™¨å¹¶äº§å‡ºé”™è¯¯
        if isinstance(result, dict) and result.get("status") == "error":
            return self._create_single_error_generator(result)

        return result

        # # è‡ªåŠ¨ç»§ç»­æ‰§è¡Œ
        # return self._auto_continue_generator(
        #     thread_id,
        #     system_prompt,
        #     llm_model,
        #     llm_temperature,
        #     llm_max_tokens,
        #     tool_choice,
        #     config,
        #     stream,
        #     auto_continue_state,
        #     temporary_message,
        #     native_max_auto_continues,
        #     latest_user_message_content,
        #     cancellation_event,
        # )

    async def _execute_run(
        self,
        thread_id: str,
        system_prompt: dict[str, Any],
        llm_model: str,
        tool_choice: ToolChoice,
        config: ProcessorConfig,
        stream: bool,
        temporary_message: Optional[dict[str, Any]] = None,
        latest_user_message_content: Optional[str] = None,
        cancellation_event: Optional[asyncio.Event] = None,
    ) -> Union[dict[str, Any], AsyncGenerator]:
        """æ‰§è¡Œå•æ¬¡LLMè¿è¡Œã€‚"""

        # å…³é”®: ç¡®ä¿configå§‹ç»ˆæ˜¯ProcessorConfigå¯¹è±¡
        if not isinstance(config, ProcessorConfig):
            logger.error(f"é”™è¯¯: configæ˜¯{type(config)}ï¼ŒæœŸæœ›ProcessorConfigã€‚å€¼: {config}")
            config = ProcessorConfig()  # åˆ›å»ºæ–°å®ä¾‹ä½œä¸ºåå¤‡

        try:
            messages = await self.get_llm_messages(thread_id)
            thread = Threads.get_by_id(thread_id, Thread.session_id)
            session_id = thread.session_id if thread else None

            # è·å–LLMè°ƒç”¨çš„å·¥å…·æ¨¡å¼(åœ¨å‹ç¼©ä¹‹å)
            openapi_tool_schemas = None

            prepared_messages = messages

            # æ³¨æ„: æˆ‘ä»¬ä¸åœ¨æ­¤å¤„è®°å½•tokenè®¡æ•°ï¼Œå› ä¸ºç¼“å­˜å—ç»™å‡ºä¸å‡†ç¡®çš„è®¡æ•°
            # LLMçš„usage.prompt_tokens(åœ¨è°ƒç”¨åæŠ¥å‘Š)æ˜¯å‡†ç¡®çš„çœŸç›¸æ¥æº
            logger.info(f"ğŸ“¤ å‘LLMå‘é€ {len(prepared_messages)} æ¡å‡†å¤‡å¥½çš„æ¶ˆæ¯")

            # è°ƒç”¨LLM
            try:
                llm_response = await make_llm_api_call(
                    prepared_messages,
                    llm_model,
                    tools=openapi_tool_schemas,
                    stream=stream,
                    session_id=session_id,
                )
            except LLMError as e:
                return {"type": "status", "status": "error", "message": str(e)}

            # æ£€æŸ¥é”™è¯¯å“åº”
            if isinstance(llm_response, dict) and llm_response.get("status") == "error":
                return llm_response

            if stream and hasattr(llm_response, "__aiter__"):
                return self.response_processor.process_streaming_response(
                    cast(AsyncGenerator, llm_response),
                    thread_id,
                    prepared_messages,
                    llm_model,
                    config,
                    cancellation_event,
                )
            else:
                return self.response_processor.process_non_streaming_response(
                    llm_response,
                    thread_id,
                    prepared_messages,
                    llm_model,
                    config,
                )

        except Exception as e:
            processed_error = ErrorProcessor.process_system_error(e, context={"thread_id": thread_id})
            ErrorProcessor.log_error(processed_error)
            return processed_error.to_stream_dict()

    async def _auto_continue_generator(
        self,
        thread_id: str,
        system_prompt: dict[str, Any],
        llm_model: str,
        llm_temperature: float,
        llm_max_tokens: Optional[int],
        tool_choice: ToolChoice,
        config: ProcessorConfig,
        stream: bool,
        auto_continue_state: dict[str, Any],
        temporary_message: Optional[dict[str, Any]],
        native_max_auto_continues: int,
        latest_user_message_content: Optional[str] = None,
        cancellation_event: Optional[asyncio.Event] = None,
    ) -> AsyncGenerator:
        """å¤„ç†è‡ªåŠ¨ç»§ç»­é€»è¾‘çš„ç”Ÿæˆå™¨ã€‚"""

        while auto_continue_state["active"] and auto_continue_state["count"] < native_max_auto_continues:
            auto_continue_state["active"] = False  # é‡ç½®æœ¬æ¬¡è¿­ä»£

            try:
                # ç»§ç»­å‰æ£€æŸ¥å–æ¶ˆä¿¡å·
                if cancellation_event and cancellation_event.is_set():
                    logger.info(f"çº¿ç¨‹ {thread_id} çš„è‡ªåŠ¨ç»§ç»­ç”Ÿæˆå™¨æ”¶åˆ°å–æ¶ˆä¿¡å·")
                    break

                response_gen = await self._execute_run(
                    thread_id,
                    system_prompt,
                    llm_model,
                    tool_choice,
                    config,
                    stream,
                    # auto_continue_state,
                    temporary_message if auto_continue_state["count"] == 0 else None,
                    latest_user_message_content if auto_continue_state["count"] == 0 else None,
                    cancellation_event,
                )

                # å¤„ç†é”™è¯¯å“åº”
                if isinstance(response_gen, dict) and response_gen.get("status") == "error":
                    yield response_gen
                    break

                # å¤„ç†æµå¼å“åº”
                if hasattr(response_gen, "__aiter__"):
                    async for chunk in cast(AsyncGenerator, response_gen):
                        # æ£€æŸ¥å–æ¶ˆä¿¡å·
                        if cancellation_event and cancellation_event.is_set():
                            logger.info(f"å¤„ç†çº¿ç¨‹ {thread_id} è‡ªåŠ¨ç»§ç»­æµæ—¶æ”¶åˆ°å–æ¶ˆä¿¡å·")
                            break

                        # æ£€æŸ¥è‡ªåŠ¨ç»§ç»­è§¦å‘å™¨
                        should_continue = self._check_auto_continue_trigger(
                            chunk, auto_continue_state, native_max_auto_continues
                        )

                        # è·³è¿‡è§¦å‘è‡ªåŠ¨ç»§ç»­çš„å®Œæˆå—(ä½†ä¸æ˜¯å·¥å…·æ‰§è¡Œï¼Œå‰ç«¯éœ€è¦é‚£äº›)
                        if should_continue:
                            if chunk.get("type") == "status":
                                try:
                                    content = json.loads(chunk.get("content", "{}"))
                                    # ä»…è·³è¿‡é•¿åº¦é™åˆ¶å®ŒæˆçŠ¶æ€(å‰ç«¯éœ€è¦å·¥å…·æ‰§è¡Œå®Œæˆ)
                                    if content.get("finish_reason") == "length":
                                        continue
                                except (json.JSONDecodeError, TypeError):
                                    pass

                        yield chunk
                else:
                    yield response_gen

                if not auto_continue_state["active"]:
                    break

            except Exception as e:
                processed_error = ErrorProcessor.process_system_error(e, context={"thread_id": thread_id})
                ErrorProcessor.log_error(processed_error)
                yield processed_error.to_stream_dict()
                return

        # å¤„ç†è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        if auto_continue_state["active"] and auto_continue_state["count"] >= native_max_auto_continues:
            logger.warning(f"è¾¾åˆ°æœ€å¤§è‡ªåŠ¨ç»§ç»­é™åˆ¶ ({native_max_auto_continues})")
            yield {
                "type": "content",
                "content": f"\n[Agentè¾¾åˆ°æœ€å¤§è‡ªåŠ¨ç»§ç»­é™åˆ¶ {native_max_auto_continues}]",
            }

    def _check_auto_continue_trigger(
        self,
        chunk: dict[str, Any],
        auto_continue_state: dict[str, Any],
        native_max_auto_continues: int,
    ) -> bool:
        """æ£€æŸ¥å“åº”å—æ˜¯å¦åº”è¯¥è§¦å‘è‡ªåŠ¨ç»§ç»­ã€‚"""
        if chunk.get("type") == "status":
            try:
                content = (
                    json.loads(chunk.get("content", "{}"))
                    if isinstance(chunk.get("content"), str)
                    else chunk.get("content", {})
                )
                finish_reason = content.get("finish_reason")
                tools_executed = content.get("tools_executed", False)

                # ä¸ºä»¥ä¸‹æƒ…å†µè§¦å‘è‡ªåŠ¨ç»§ç»­: åŸç”Ÿå·¥å…·è°ƒç”¨ã€é•¿åº¦é™åˆ¶æˆ–XMLå·¥å…·å·²æ‰§è¡Œ
                if finish_reason == "tool_calls" or tools_executed:
                    if native_max_auto_continues > 0:
                        logger.debug(
                            f"å› å·¥å…·æ‰§è¡Œè‡ªåŠ¨ç»§ç»­ ({auto_continue_state['count'] + 1}/{native_max_auto_continues})"
                        )
                        auto_continue_state["active"] = True
                        auto_continue_state["count"] += 1
                        return True
                elif finish_reason == "length":
                    logger.debug(f"å› é•¿åº¦é™åˆ¶è‡ªåŠ¨ç»§ç»­ ({auto_continue_state['count'] + 1}/{native_max_auto_continues})")
                    auto_continue_state["active"] = True
                    auto_continue_state["count"] += 1
                    return True
                elif finish_reason == "xml_tool_limit_reached":
                    logger.debug("å› XMLå·¥å…·é™åˆ¶åœæ­¢è‡ªåŠ¨ç»§ç»­")
                    auto_continue_state["active"] = False
            except (json.JSONDecodeError, TypeError):
                pass

        return False

    async def _create_single_error_generator(self, error_dict: dict[str, Any]):
        """åˆ›å»ºäº§å‡ºå•ä¸ªé”™è¯¯æ¶ˆæ¯çš„å¼‚æ­¥ç”Ÿæˆå™¨ã€‚"""
        yield error_dict
