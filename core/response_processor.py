import asyncio
import json
import uuid
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import (
    Any,
    AsyncGenerator,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    Union,
    cast,
)

from claude_agent_sdk import SystemMessage
from claude_agent_sdk.types import (
    AssistantMessage,
    TextBlock,
    ToolResultBlock,
    ToolUseBlock,
    UserMessage,
)
from loguru import logger

from configs import app_config
from core.error_processor import ErrorProcessor
from core.tool import ToolResult
from core.utils.json_helpers import (
    format_for_yield,
    safe_json_parse,
    to_json_string,
)
from models.thread import Threads

# XMLç»“æžœæ·»åŠ ç­–ç•¥çš„ç±»åž‹åˆ«å
XmlAddingStrategy = Literal["user_message", "assistant_message", "inline_edit"]


@dataclass
class ToolExecutionContext:
    """å·¥å…·æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ŒåŒ…å«è°ƒç”¨è¯¦æƒ…ã€ç»“æžœå’Œæ˜¾ç¤ºä¿¡æ¯ã€‚"""

    tool_call: Dict[str, Any]
    tool_index: int
    result: Optional[ToolResult] = None
    function_name: Optional[str] = None
    error: Optional[Exception] = None
    assistant_message_id: Optional[str] = None


@dataclass
class ProcessorConfig:
    """
    å“åº”å¤„ç†å’Œå·¥å…·æ‰§è¡Œçš„é…ç½®ã€‚

    è¯¥ç±»æŽ§åˆ¶LLMå“åº”çš„å¤„ç†æ–¹å¼ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨çš„æ£€æµ‹ã€æ‰§è¡Œå’Œç»“æžœå¤„ç†ã€‚

    å±žæ€§:
        execute_on_stream: å¯¹äºŽæµå¼å“åº”ï¼Œæ˜¯å³æ—¶æ‰§è¡Œå·¥å…·è¿˜æ˜¯åœ¨ç»“æŸæ—¶æ‰§è¡Œ
    """

    execute_on_stream: bool = False


class ResponseProcessor:
    """å¤„ç†LLMå“åº”ï¼Œæå–å¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨ã€‚"""

    def __init__(
        self,
        add_message_callback: Callable,
        agent_config: Optional[dict] = None,
    ):
        """åˆå§‹åŒ–ResponseProcessorã€‚

        å‚æ•°:
            add_message_callback: å‘çº¿ç¨‹æ·»åŠ æ¶ˆæ¯çš„å›žè°ƒå‡½æ•°ã€‚
                å¿…é¡»è¿”å›žå®Œæ•´ä¿å­˜çš„æ¶ˆæ¯å¯¹è±¡(dict)æˆ–Noneã€‚
            agent_config: å¯é€‰çš„ä»£ç†é…ç½®ï¼ŒåŒ…å«ç‰ˆæœ¬ä¿¡æ¯
        """
        self.add_message = add_message_callback

        self.agent_config = agent_config

    async def _yield_message(
        self, message_obj: Optional[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        """è¾…åŠ©æ–¹æ³•ï¼šä»¥é€‚å½“çš„æ ¼å¼ç”Ÿæˆæ¶ˆæ¯ã€‚

        ç¡®ä¿å†…å®¹å’Œå…ƒæ•°æ®ä¸ºJSONå­—ç¬¦ä¸²ï¼Œä»¥ä¿è¯å®¢æˆ·ç«¯å…¼å®¹æ€§ã€‚
        """
        if message_obj:
            return format_for_yield(message_obj)
        return None

    def _serialize_model_response(self, model_response) -> Dict[str, Any]:
        """å°†ModelResponseå¯¹è±¡è½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„å­—å…¸ã€‚

        å‚æ•°:
            model_response: ModelResponseå¯¹è±¡

        è¿”å›ž:
            ModelResponseçš„å­—å…¸è¡¨ç¤º
        """
        try:
            # å°è¯•ä½¿ç”¨model_dumpæ–¹æ³•ï¼ˆPydantic v2ï¼‰
            if hasattr(model_response, "model_dump"):
                return model_response.model_dump()

            # å°è¯•ä½¿ç”¨dictæ–¹æ³•ï¼ˆPydantic v1ï¼‰
            elif hasattr(model_response, "dict"):
                return model_response.dict()

            # å›žé€€ï¼šæ‰‹åŠ¨æå–å¸¸è§å±žæ€§
            else:
                result = {}

                # å¸¸è§çš„LiteLLM ModelResponseå±žæ€§
                for attr in [
                    "id",
                    "object",
                    "created",
                    "model",
                    "choices",
                    "usage",
                    "system_fingerprint",
                ]:
                    if hasattr(model_response, attr):
                        value = getattr(model_response, attr)
                        # é€’å½’å¤„ç†åµŒå¥—å¯¹è±¡
                        if hasattr(value, "model_dump"):
                            result[attr] = value.model_dump()
                        elif hasattr(value, "dict"):
                            result[attr] = value.dict()
                        elif isinstance(value, list):
                            result[attr] = [
                                item.model_dump()
                                if hasattr(item, "model_dump")
                                else item.dict()
                                if hasattr(item, "dict")
                                else item
                                for item in value
                            ]
                        else:
                            result[attr] = value

                return result

        except Exception as e:
            logger.warning(f"åºåˆ—åŒ–ModelResponseå¤±è´¥: {str(e)}, å›žé€€åˆ°å­—ç¬¦ä¸²è¡¨ç¤º")
            # æœ€ç»ˆå›žé€€ï¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            return {"raw_response": str(model_response), "serialization_error": str(e)}

    async def _add_message_with_agent_info(
        self,
        thread_id: str,
        type: str,
        content: Union[Dict[str, Any], List[Any], str],
        is_llm_message: bool = False,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """è¾…åŠ©æ–¹æ³•ï¼šåœ¨å¯ç”¨æ—¶æ·»åŠ åŒ…å«ä»£ç†ç‰ˆæœ¬ä¿¡æ¯çš„æ¶ˆæ¯ã€‚"""
        agent_id = None
        agent_version_id = None

        if self.agent_config:
            agent_id = self.agent_config.get("agent_id")
            agent_version_id = self.agent_config.get("current_version_id")

        return await self.add_message(
            thread_id=thread_id,
            type=type,
            content=content,
            is_llm_message=is_llm_message,
            metadata=metadata,
            agent_id=agent_id,
            agent_version_id=agent_version_id,
        )

    async def process_streaming_response(
        self,
        llm_response: AsyncGenerator,
        thread_id: str,
        prompt_messages: List[Dict[str, Any]],
        llm_model: str,
        config: ProcessorConfig = ProcessorConfig(),
        cancellation_event: Optional[asyncio.Event] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """å¤„ç†æµå¼LLMå“åº”ï¼Œå¤„ç†å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œã€‚

        å‚æ•°:
            llm_response: LLMçš„æµå¼å“åº”
            thread_id: å¯¹è¯çº¿ç¨‹çš„ID
            prompt_messages: å‘é€ç»™LLMçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæç¤ºï¼‰
            llm_model: ä½¿ç”¨çš„LLMæ¨¡åž‹åç§°
            config: è§£æžå’Œæ‰§è¡Œçš„é…ç½®
            cancellation_event: å–æ¶ˆäº‹ä»¶

        ç”Ÿæˆ:
            å®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡ï¼ŒåŒ¹é…æ•°æ®åº“æ¨¡å¼ï¼Œé™¤äº†å†…å®¹å—ã€‚
        """
        logger.info(f"ðŸš€ å¼€å§‹å¤„ç†Claude Codeæµå¼å“åº” - thread: {thread_id}")

        # åˆå§‹åŒ–å–æ¶ˆäº‹ä»¶
        if cancellation_event is None:
            cancellation_event = asyncio.Event()

        # åˆå§‹åŒ–çŠ¶æ€å˜é‡
        accumulated_content = ""  # ç´¯ç§¯çš„æ–‡æœ¬å†…å®¹
        content_blocks = {}  # index -> block_data (textæˆ–tool_use)
        current_message_id = None  # å½“å‰assistantæ¶ˆæ¯ID
        usage_data = {}  # Tokenä½¿ç”¨ç»Ÿè®¡
        finish_reason = None  # å®ŒæˆåŽŸå› 
        last_assistant_message_object = None  # æœ€åŽä¿å­˜çš„assistantæ¶ˆæ¯å¯¹è±¡
        turn_count = 0  # å¯¹è¯è½®æ¬¡è®¡æ•°
        session_id = None  # claude code ä¼šè¯ID

        # å­˜å‚¨å®Œæ•´çš„å“åº”å¯¹è±¡ç”¨äºŽbilling
        final_llm_response = None
        first_chunk_time = None
        last_chunk_time = None
        llm_response_end_saved = False

        # ç”Ÿæˆè¿è¡ŒID
        thread_run_id = str(uuid.uuid4())
        llm_response_id = str(uuid.uuid4())

        logger.info(
            f"ðŸ“ è¿è¡ŒID: thread_run_id={thread_run_id}, llm_response_id={llm_response_id}"
        )

        try:
            # --- ä¿å­˜å¹¶yieldå¯åŠ¨äº‹ä»¶ ---
            start_content = {
                "status_type": "thread_run_start",
                "thread_run_id": thread_run_id,
            }
            start_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=start_content,
                is_llm_message=False,
                metadata={"thread_run_id": thread_run_id},
            )
            if start_msg_obj:
                yield format_for_yield(start_msg_obj)

            llm_start_content = {
                "llm_response_id": llm_response_id,
                "model": llm_model,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            llm_start_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="llm_response_start",
                content=llm_start_content,
                is_llm_message=False,
                metadata={
                    "thread_run_id": thread_run_id,
                    "llm_response_id": llm_response_id,
                },
            )
            if llm_start_msg_obj:
                yield format_for_yield(llm_start_msg_obj)
                logger.info("âœ… å·²ä¿å­˜llm_response_start")
            # --- å¯åŠ¨äº‹ä»¶ç»“æŸ ---

            __sequence = 0  # æ¶ˆæ¯åºåˆ—å·

            # è®¾ç½®debugæ–‡ä»¶ä¿å­˜ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
            debug_file = None
            debug_file_json = None
            raw_chunks_data = []  # å­˜å‚¨æ‰€æœ‰chunkæ•°æ®ç”¨äºŽJSONLå¯¼å‡º

            if app_config.DEBUG:
                debug_dir = Path("debug_streams")
                debug_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                debug_file = debug_dir / f"stream_{thread_id[:8]}_{timestamp}.txt"
                debug_file_json = (
                    debug_dir / f"stream_{thread_id[:8]}_{timestamp}.jsonl"
                )

                logger.info(f"ðŸ“ ä¿å­˜åŽŸå§‹æµè¾“å‡ºåˆ°: {debug_file}")

            chunk_count = 0
            tool_index = 0  # å·¥å…·è°ƒç”¨ç´¢å¼•

            # --- ä¸»å¾ªçŽ¯ï¼šå¤„ç†Claude Codeæµå¼å“åº” ---
            async for chunk in llm_response:
                # æ£€æŸ¥å–æ¶ˆä¿¡å·
                if cancellation_event.is_set():
                    logger.info(f"âš ï¸ æ”¶åˆ°å–æ¶ˆä¿¡å·ï¼Œåœæ­¢å¤„ç† - thread: {thread_id}")
                    finish_reason = "cancelled"
                    break

                chunk_count += 1

                # è·Ÿè¸ªæ—¶é—´
                current_time = datetime.now().timestamp()
                if first_chunk_time is None:
                    first_chunk_time = current_time
                last_chunk_time = current_time

                # èŽ·å–chunkç±»åž‹
                chunk_type = type(chunk).__name__

                # å®šæœŸè®°å½•æ—¥å¿—
                if chunk_count == 1 or (chunk_count % 100 == 0):
                    logger.debug(f"å¤„ç†chunk #{chunk_count}, type={chunk_type}")

                # ä¿å­˜åŽŸå§‹chunkæ•°æ®ç”¨äºŽè°ƒè¯•ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
                if app_config.DEBUG:
                    try:
                        chunk_data = {
                            "chunk_num": chunk_count,
                            "timestamp": current_time,
                            "chunk_type": chunk_type,
                            "chunk_str": str(chunk)[:200],  # å‰200å­—ç¬¦
                        }
                        raw_chunks_data.append(chunk_data)

                        # å¢žé‡å†™å…¥JSONLæ–‡ä»¶
                        with open(debug_file_json, "a", encoding="utf-8") as f:
                            f.write(json.dumps(chunk_data, ensure_ascii=False) + "\n")
                    except Exception as e:
                        logger.debug(f"ä¿å­˜chunkæ•°æ®é”™è¯¯: {e}")

                # --- 1. å¤„ç† SystemMessageï¼ˆåˆå§‹åŒ–ä¿¡æ¯ï¼‰ ---
                if chunk_type == "SystemMessage":
                    logger.debug("ðŸ“‹ æ”¶åˆ°ç³»ç»Ÿåˆå§‹åŒ–æ¶ˆæ¯")
                    system_message = cast(SystemMessage, chunk)
                    session_id = system_message.data.get("session_id")
                    if session_id:
                        Threads.update_session_id(thread_id, session_id)
                    continue

                # --- 2. å¤„ç† StreamEventï¼ˆæµå¼äº‹ä»¶ï¼‰ ---
                elif chunk_type == "StreamEvent":
                    event = chunk.event
                    event_type = event.get("type")

                    # 2.1 message_start - è®°å½•message_idå’Œusage
                    if event_type == "message_start":
                        turn_count += 1
                        current_message_id = event["message"]["id"]
                        if "usage" in event["message"]:
                            usage_data = event["message"]["usage"]
                        logger.info(
                            f"ðŸ“ å¼€å§‹ç¬¬{turn_count}è½®æ¶ˆæ¯: {current_message_id}"
                        )

                    # 2.2 content_block_start - æ–‡æœ¬å—æˆ–å·¥å…·è°ƒç”¨å—å¼€å§‹
                    elif event_type == "content_block_start":
                        index = event["index"]
                        content_block = event["content_block"]
                        block_type = content_block["type"]

                        if block_type == "text":
                            # æ–‡æœ¬å—å¼€å§‹
                            content_blocks[index] = {"type": "text", "text": ""}
                            logger.debug(f"ðŸ“„ æ–‡æœ¬å—å¼€å§‹ (index={index})")

                        elif block_type == "tool_use":
                            # å·¥å…·è°ƒç”¨å—å¼€å§‹
                            tool_call_id = content_block["id"]
                            tool_name = content_block["name"]
                            content_blocks[index] = {
                                "type": "tool_use",
                                "id": tool_call_id,
                                "name": tool_name,
                                "input": "",  # å°†ç´¯ç§¯JSONç‰‡æ®µ
                            }
                            logger.info(
                                f"ðŸ”§ å·¥å…·è°ƒç”¨å¼€å§‹: {tool_name} (id={tool_call_id})"
                            )

                            # yield tool_started çŠ¶æ€æ¶ˆæ¯
                            tool_started_content = {
                                "status_type": "tool_started",
                                "tool_call_id": tool_call_id,
                                "function_name": tool_name,
                                "tool_index": tool_index,
                            }
                            tool_started_msg = await self.add_message(
                                thread_id=thread_id,
                                type="status",
                                content=tool_started_content,
                                is_llm_message=False,
                                metadata={"thread_run_id": thread_run_id},
                            )
                            if tool_started_msg:
                                yield format_for_yield(tool_started_msg)

                            tool_index += 1

                    # 2.3 content_block_delta - å†…å®¹å¢žé‡
                    elif event_type == "content_block_delta":
                        index = event["index"]
                        delta = event["delta"]
                        delta_type = delta["type"]

                        if delta_type == "text_delta":
                            # æ–‡æœ¬å¢žé‡
                            text_chunk = delta["text"]
                            accumulated_content += text_chunk
                            if (
                                index in content_blocks
                                and content_blocks[index]["type"] == "text"
                            ):
                                content_blocks[index]["text"] += text_chunk

                            # yieldæ–‡æœ¬å†…å®¹
                            now_chunk = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            yield {
                                "sequence": __sequence,
                                "message_id": None,
                                "thread_id": thread_id,
                                "type": "assistant",
                                "is_llm_message": True,
                                "content": to_json_string(
                                    {"role": "assistant", "content": text_chunk}
                                ),
                                "metadata": to_json_string(
                                    {
                                        "stream_status": "chunk",
                                        "thread_run_id": thread_run_id,
                                    }
                                ),
                                "created_at": now_chunk,
                                "updated_at": now_chunk,
                            }
                            __sequence += 1

                        elif delta_type == "input_json_delta":
                            # å·¥å…·å‚æ•°JSONå¢žé‡
                            partial_json = delta["partial_json"]
                            if (
                                index in content_blocks
                                and content_blocks[index]["type"] == "tool_use"
                            ):
                                content_blocks[index]["input"] += partial_json

                    # 2.4 content_block_stop - å†…å®¹å—ç»“æŸ
                    elif event_type == "content_block_stop":
                        index = event["index"]
                        block = content_blocks.get(index)

                        if block and block["type"] == "tool_use":
                            # å·¥å…·è°ƒç”¨å®Œæ•´ï¼Œè§£æžå‚æ•°
                            try:
                                tool_input = json.loads(block["input"])
                                block["parsed_input"] = tool_input
                                logger.debug(f"âœ… å·¥å…·å‚æ•°è§£æžå®Œæˆ: {block['name']}")
                            except json.JSONDecodeError as e:
                                logger.error(f"âŒ å·¥å…·å‚æ•°JSONè§£æžå¤±è´¥: {e}")
                                block["parsed_input"] = {}

                    # 2.5 message_delta - æ¶ˆæ¯å¢žé‡ï¼ˆusageå’Œstop_reasonï¼‰
                    elif event_type == "message_delta":
                        delta = event.get("delta", {})
                        if "stop_reason" in delta:
                            finish_reason = delta["stop_reason"]
                            logger.debug(f"ðŸ“Œ finish_reason={finish_reason}")
                        if "usage" in event:
                            usage_data.update(event["usage"])

                    # 2.6 message_stop - æ¶ˆæ¯ç»“æŸ
                    elif event_type == "message_stop":
                        logger.debug(f"âœ… æ¶ˆæ¯æµç»“æŸ (ç¬¬{turn_count}è½®)")

                # --- 3. å¤„ç† AssistantMessageï¼ˆå®Œæ•´æ¶ˆæ¯ï¼‰ ---
                elif chunk_type == "AssistantMessage":
                    # ä¿å­˜assistantæ¶ˆæ¯åˆ°DB
                    content_data = chunk.content
                    message_content = self._format_assistant_message_content(
                        content_data
                    )

                    last_assistant_message_object = (
                        await self._add_message_with_agent_info(
                            thread_id=thread_id,
                            type="assistant",
                            content=message_content,
                            is_llm_message=True,
                            metadata={"thread_run_id": thread_run_id},
                        )
                    )

                    if last_assistant_message_object:
                        # yieldå®Œæ•´æ¶ˆæ¯
                        yield_metadata = last_assistant_message_object.get(
                            "metadata", {}
                        )
                        yield_metadata["stream_status"] = "complete"
                        yield_message = last_assistant_message_object.copy()
                        yield_message["metadata"] = yield_metadata
                        yield format_for_yield(yield_message)

                        logger.info(
                            f"âœ… å·²ä¿å­˜assistantæ¶ˆæ¯: {last_assistant_message_object.get('message_id')}"
                        )

                # --- 4. å¤„ç† UserMessageï¼ˆå·¥å…·æ‰§è¡Œç»“æžœï¼‰ ---
                elif chunk_type == "UserMessage":
                    # æå–å·¥å…·ç»“æžœï¼ˆcontentæ˜¯ToolResultBlockåˆ—è¡¨ï¼‰
                    for block in chunk.content:
                        if hasattr(block, "tool_use_id"):
                            tool_result_content = {
                                "tool_use_id": block.tool_use_id,
                                "content": block.content,
                                "is_error": getattr(block, "is_error", None),
                            }

                            # ä¿å­˜tool resultåˆ°DB
                            tool_result_msg = await self._add_message_with_agent_info(
                                thread_id=thread_id,
                                type="tool_result",
                                content=tool_result_content,
                                is_llm_message=False,
                                metadata={"thread_run_id": thread_run_id},
                            )

                            # yield tool_completedçŠ¶æ€
                            if tool_result_msg:
                                # yieldå·¥å…·å®ŒæˆçŠ¶æ€
                                tool_completed_content = {
                                    "status_type": "tool_completed",
                                    "tool_call_id": block.tool_use_id,
                                    "success": not getattr(block, "is_error", False),
                                }
                                tool_completed_msg = await self.add_message(
                                    thread_id=thread_id,
                                    type="status",
                                    content=tool_completed_content,
                                    is_llm_message=False,
                                    metadata={"thread_run_id": thread_run_id},
                                )
                                if tool_completed_msg:
                                    yield format_for_yield(tool_completed_msg)

                                # yieldå·¥å…·ç»“æžœæ¶ˆæ¯
                                yield format_for_yield(tool_result_msg)

                            logger.info(f"âœ… å·²ä¿å­˜å·¥å…·ç»“æžœ: {block.tool_use_id}")

                # --- 5. å¤„ç† ResultMessageï¼ˆæœ€ç»ˆç»“æžœï¼‰ ---
                elif chunk_type == "ResultMessage":
                    # æå–ç»Ÿè®¡ä¿¡æ¯
                    final_usage = chunk.usage if hasattr(chunk, "usage") else usage_data
                    total_cost = getattr(chunk, "total_cost_usd", 0)
                    num_turns = getattr(chunk, "num_turns", turn_count)

                    logger.info(f"ðŸŽ‰ å¯¹è¯å®Œæˆ: {num_turns}è½®, æˆæœ¬=${total_cost:.5f}")
                    logger.info(f"ðŸ“Š Tokenä½¿ç”¨: {final_usage}")

                    # ä¿å­˜åˆ°final_llm_responseä»¥ä¾¿åŽç»­ä¿å­˜llm_response_end
                    final_llm_response = chunk

            # --- æµå¤„ç†ç»“æŸ ---
            logger.info(
                f"âœ… æµå¤„ç†å®Œæˆ. æ€»chunks: {chunk_count}, finish_reason: {finish_reason}"
            )
            logger.info(
                f"ðŸ“ ç´¯ç§¯å†…å®¹é•¿åº¦: {len(accumulated_content)} å­—ç¬¦, å¯¹è¯è½®æ•°: {turn_count}"
            )

            # ä¿å­˜debugæ‘˜è¦ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
            if app_config.DEBUG:
                try:
                    summary = {
                        "thread_id": thread_id,
                        "thread_run_id": thread_run_id,
                        "total_chunks": chunk_count,
                        "turn_count": turn_count,
                        "finish_reason": finish_reason,
                        "accumulated_content_length": len(accumulated_content),
                        "tool_calls_count": len(
                            [
                                b
                                for b in content_blocks.values()
                                if b.get("type") == "tool_use"
                            ]
                        ),
                        "first_chunk_time": first_chunk_time,
                        "last_chunk_time": last_chunk_time,
                        "final_usage": usage_data,
                    }

                    # è®¡ç®—å“åº”æ—¶é—´
                    if first_chunk_time and last_chunk_time:
                        summary["response_time_ms"] = (
                            last_chunk_time - first_chunk_time
                        ) * 1000

                    # å†™å…¥æ‘˜è¦åˆ°æ–‡æœ¬æ–‡ä»¶
                    with open(debug_file, "w", encoding="utf-8") as f:
                        f.write("=" * 80 + "\n")
                        f.write("CLAUDE CODE STREAM DEBUG SUMMARY\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(
                            json.dumps(summary, indent=2, ensure_ascii=False) + "\n\n"
                        )
                        f.write("=" * 80 + "\n")
                        f.write("ACCUMULATED CONTENT\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(accumulated_content + "\n\n")
                        f.write("=" * 80 + "\n")
                        f.write(f"Total chunks: {chunk_count}\n")
                        f.write(f"Content blocks: {len(content_blocks)}\n")

                    logger.info(
                        f"âœ… å·²ä¿å­˜stream debugæ–‡ä»¶: {debug_file} å’Œ {debug_file_json}"
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜stream debugæ‘˜è¦é”™è¯¯: {e}")

            # å¦‚æžœæœ‰æ—¶é—´æ•°æ®ï¼Œè®¡ç®—å“åº”æ—¶é—´
            response_ms = None
            if first_chunk_time and last_chunk_time:
                response_ms = (last_chunk_time - first_chunk_time) * 1000

            # éªŒè¯usageå·²æ•èŽ·
            if not usage_data:
                logger.warning("âš ï¸ æœªä»Žæµä¸­æ•èŽ·usageæ•°æ®")

            # --- yield finishçŠ¶æ€ ---
            if finish_reason:
                finish_content = {
                    "status_type": "finish",
                    "finish_reason": finish_reason,
                }
                finish_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=finish_content,
                    is_llm_message=False,
                    metadata={"thread_run_id": thread_run_id},
                )
                if finish_msg_obj:
                    yield format_for_yield(finish_msg_obj)
                logger.info(f"âœ… yield finishçŠ¶æ€: {finish_reason}")

            # --- ä¿å­˜å¹¶yield llm_response_end ---
            if last_assistant_message_object:
                try:
                    # æž„å»ºllm_response_endå†…å®¹
                    logger.info("âœ… æž„å»ºClaude Code llm_response_end")
                    llm_end_content = self._serialize_claude_code_response(
                        final_llm_response, usage_data
                    )

                    # æ·»åŠ streamingæ ‡å¿—å’Œå“åº”æ—¶é—´
                    llm_end_content["streaming"] = True
                    if response_ms:
                        llm_end_content["response_ms"] = response_ms
                    llm_end_content["llm_response_id"] = llm_response_id

                    # ä¿å­˜llm_response_endæ¶ˆæ¯
                    llm_end_msg_obj = await self.add_message(
                        thread_id=thread_id,
                        type="llm_response_end",
                        content=llm_end_content,
                        is_llm_message=False,
                        metadata={
                            "thread_run_id": thread_run_id,
                            "llm_response_id": llm_response_id,
                        },
                    )
                    llm_response_end_saved = True
                    # Yieldåˆ°streamç”¨äºŽå®žæ—¶æ›´æ–°
                    if llm_end_msg_obj:
                        yield format_for_yield(llm_end_msg_obj)
                    logger.info("âœ… llm_response_endå·²ä¿å­˜")
                except Exception as e:
                    logger.error(f"ä¿å­˜llm_response_endé”™è¯¯: {str(e)}")

        except Exception as e:
            # ä½¿ç”¨ErrorProcessorè¿›è¡Œä¸€è‡´çš„é”™è¯¯å¤„ç†
            processed_error = ErrorProcessor.process_system_error(
                e, context={"thread_id": thread_id}
            )
            ErrorProcessor.log_error(processed_error)

            # ä¿å­˜å¹¶ç”Ÿæˆé”™è¯¯çŠ¶æ€æ¶ˆæ¯
            err_content = {
                "role": "system",
                "status_type": "error",
                "message": processed_error.message,
            }
            err_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=err_content,
                is_llm_message=False,
                metadata={
                    "thread_run_id": thread_run_id
                    if "thread_run_id" in locals()
                    else None
                },
            )
            if err_msg_obj:
                yield format_for_yield(err_msg_obj)
            raise

        finally:
            # IMPORTANT: Finallyå—å³ä½¿åœ¨streamåœæ­¢æ—¶ä¹Ÿä¼šè¿è¡Œï¼ˆGeneratorExitï¼‰
            # ä¸èƒ½åœ¨è¿™é‡Œyield - åªèƒ½é™é»˜åœ°ä¿å­˜åˆ°DBç”¨äºŽbilling/usageè·Ÿè¸ª

            # é˜¶æ®µ3ï¼šèµ„æºæ¸…ç† - å–æ¶ˆpendingä»»åŠ¡å¹¶å…³é—­generator
            try:
                # å°è¯•å…³é—­LLMå“åº”generatorï¼ˆå¦‚æžœæ”¯æŒaclose()ï¼‰
                # è¿™æœ‰åŠ©äºŽåœæ­¢åº•å±‚çš„HTTPè¿žæŽ¥
                if hasattr(llm_response, "aclose"):
                    try:
                        await llm_response.aclose()
                        logger.debug(f"å·²å…³é—­LLMå“åº”generator - thread: {thread_id}")
                    except Exception as close_err:
                        logger.debug(
                            f"å…³é—­LLMå“åº”generatoré”™è¯¯ï¼ˆå¯èƒ½ä¸æ”¯æŒacloseï¼‰: {close_err}"
                        )
                elif hasattr(llm_response, "close"):
                    try:
                        llm_response.close()
                        logger.debug(
                            f"å·²å…³é—­LLMå“åº”generator (sync close) - thread: {thread_id}"
                        )
                    except Exception as close_err:
                        logger.debug(f"å…³é—­LLMå“åº”generatoré”™è¯¯ (sync): {close_err}")
            except Exception as cleanup_err:
                logger.warning(f"èµ„æºæ¸…ç†é”™è¯¯: {cleanup_err}")

            # Billingä¿æŠ¤ï¼šå¦‚æžœllm_response_endè¿˜æ²¡ä¿å­˜ï¼Œåœ¨finallyå—ä¸­ä¿å­˜
            if not llm_response_end_saved and last_assistant_message_object:
                try:
                    logger.info(
                        "ðŸ’° BULLETPROOF BILLING: åœ¨finallyå—ä¸­ä¿å­˜llm_response_end"
                    )
                    if final_llm_response and usage_data:
                        logger.info("ðŸ’° ä½¿ç”¨LLMå“åº”ä¸­çš„ç²¾ç¡®usage")
                        llm_end_content = self._serialize_claude_code_response(
                            final_llm_response, usage_data
                        )
                    else:
                        logger.warning("ðŸ’° æ²¡æœ‰LLMå“åº”ä½¿ç”¨é‡ - ä¸ºè®¡è´¹ä¼°ç®—tokenä½¿ç”¨é‡")
                        llm_end_content = {"model": llm_model, "usage": {}}

                    llm_end_content["streaming"] = True
                    llm_end_content["llm_response_id"] = llm_response_id

                    response_ms = None
                    if first_chunk_time and last_chunk_time:
                        response_ms = int((last_chunk_time - first_chunk_time) * 1000)
                        llm_end_content["response_ms"] = response_ms

                    usage_info = llm_end_content.get("usage", {})
                    is_estimated = usage_info.get("estimated", False)

                    # ä¿å­˜ï¼ˆä¸yieldï¼‰
                    llm_end_msg_obj = await self.add_message(
                        thread_id=thread_id,
                        type="llm_response_end",
                        content=llm_end_content,
                        is_llm_message=False,
                        metadata={
                            "thread_run_id": thread_run_id,
                            "llm_response_id": llm_response_id,
                        },
                    )
                    logger.info("âœ… llm_response_endå·²åœ¨finallyå—ä¸­ä¿å­˜")
                    llm_response_end_saved = True
                except Exception as finally_err:
                    logger.error(
                        f"âŒ åœ¨finallyå—ä¸­ä¿å­˜llm_response_endå¤±è´¥: {str(finally_err)}"
                    )

            # Phase 4: ä¿å­˜å¹¶yield thread_run_endçŠ¶æ€
            # æ³¨æ„ï¼šåªåœ¨auto_continue_count == 0æ—¶ä¿å­˜thread_run_endï¼ˆå³æœ€å¤–å±‚è°ƒç”¨ï¼‰
            # Claude Codeä¸­ä¸ä½¿ç”¨auto_continueï¼Œæ‰€ä»¥å§‹ç»ˆä¿å­˜
            try:
                end_content = {"status_type": "thread_run_end"}
                end_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=end_content,
                    is_llm_message=False,
                    metadata={
                        "thread_run_id": thread_run_id
                        if "thread_run_id" in locals()
                        else None
                    },
                    session_id=session_id,
                )
                # ä¸è¦yield - finallyå—ä¸­çš„yieldä¼šå¯¼è‡´é—®é¢˜
                logger.info("âœ… thread_run_endå·²ä¿å­˜")
            except Exception as end_err:
                logger.error(f"ä¿å­˜thread_run_endé”™è¯¯: {str(end_err)}")
                # ä¸è¦re-raise - è®©ä¸»å¼‚å¸¸ä¼ æ’­

    async def process_non_streaming_response(
        self,
        llm_response: Any,
        thread_id: str,
        prompt_messages: List[Dict[str, Any]],
        llm_model: str,
        config: ProcessorConfig = ProcessorConfig(),
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """å¤„ç†éžæµå¼LLMå“åº”ï¼Œå¤„ç†å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œã€‚

        å‚æ•°:
            llm_response: LLMçš„å“åº”
            thread_id: å¯¹è¯çº¿ç¨‹çš„ID
            prompt_messages: å‘é€ç»™LLMçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæç¤ºï¼‰
            llm_model: ä½¿ç”¨çš„LLMæ¨¡åž‹åç§°
            config: è§£æžå’Œæ‰§è¡Œçš„é…ç½®

        ç”Ÿæˆ:
            åŒ¹é…æ•°æ®åº“æ¨¡å¼çš„å®Œæ•´æ¶ˆæ¯å¯¹è±¡ã€‚
        """
        content = ""
        thread_run_id = str(uuid.uuid4())
        assistant_message_object = None
        finish_reason = None
        native_tool_calls_for_message = []

        try:
            # ä¿å­˜å¹¶ç”Ÿæˆthread_run_startçŠ¶æ€æ¶ˆæ¯
            start_content = {
                "status_type": "thread_run_start",
                "thread_run_id": thread_run_id,
            }
            start_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=start_content,
                is_llm_message=False,
                metadata={"thread_run_id": thread_run_id},
            )
            if start_msg_obj:
                yield format_for_yield(start_msg_obj)

            # æå–finish_reasonã€å†…å®¹ã€å·¥å…·è°ƒç”¨
            if hasattr(llm_response, "choices") and llm_response.choices:
                if hasattr(llm_response.choices[0], "finish_reason"):
                    finish_reason = llm_response.choices[0].finish_reason
                    logger.debug(f"éžæµå¼finish_reason: {finish_reason}")
                response_message = (
                    llm_response.choices[0].message
                    if hasattr(llm_response.choices[0], "message")
                    else None
                )
                if response_message:
                    if (
                        hasattr(response_message, "content")
                        and response_message.content
                    ):
                        content = response_message.content

            # --- ä¿å­˜å¹¶ç”Ÿæˆæœ€ç»ˆåŠ©æ‰‹æ¶ˆæ¯ ---
            message_data = {
                "role": "assistant",
                "content": content,
                "tool_calls": native_tool_calls_for_message or None,
            }
            assistant_message_object = await self._add_message_with_agent_info(
                thread_id=thread_id,
                type="assistant",
                content=message_data,
                is_llm_message=True,
                metadata={"thread_run_id": thread_run_id},
            )
            if assistant_message_object:
                yield assistant_message_object
            else:
                logger.error(f"ä¸ºçº¿ç¨‹ {thread_id} ä¿å­˜éžæµå¼åŠ©æ‰‹æ¶ˆæ¯å¤±è´¥")
                err_content = {
                    "role": "system",
                    "status_type": "error",
                    "message": "ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯å¤±è´¥",
                }
                err_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=err_content,
                    is_llm_message=False,
                    metadata={"thread_run_id": thread_run_id},
                )
                if err_msg_obj:
                    yield format_for_yield(err_msg_obj)

            # --- ä¿å­˜å¹¶ç”Ÿæˆæœ€ç»ˆçŠ¶æ€ ---
            if finish_reason:
                finish_content = {
                    "status_type": "finish",
                    "finish_reason": finish_reason,
                }
                finish_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=finish_content,
                    is_llm_message=False,
                    metadata={"thread_run_id": thread_run_id},
                )
                if finish_msg_obj:
                    yield format_for_yield(finish_msg_obj)

            # --- ä¿å­˜å¹¶ç”Ÿæˆassistant_response_end ---
            if assistant_message_object:  # ä»…åœ¨ä¿å­˜äº†åŠ©æ‰‹æ¶ˆæ¯æ—¶ä¿å­˜
                try:
                    # å°†LiteLLM ModelResponseè½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„å­—å…¸
                    response_dict = self._serialize_model_response(llm_response)

                    # åœ¨å†…å®¹ä¸­ä¿å­˜åºåˆ—åŒ–çš„å“åº”å¯¹è±¡
                    await self.add_message(
                        thread_id=thread_id,
                        type="assistant_response_end",
                        content=response_dict,
                        is_llm_message=False,
                        metadata={"thread_run_id": thread_run_id},
                    )
                    logger.debug("éžæµå¼å“åº”çš„åŠ©æ‰‹å“åº”ç»“æŸå·²ä¿å­˜")
                except Exception as e:
                    logger.error(f"ä¸ºéžæµå¼ä¿å­˜åŠ©æ‰‹å“åº”ç»“æŸæ—¶å‡ºé”™: {str(e)}")

        except Exception as e:
            # ä½¿ç”¨ErrorProcessorè¿›è¡Œä¸€è‡´çš„é”™è¯¯å¤„ç†
            processed_error = ErrorProcessor.process_system_error(
                e, context={"thread_id": thread_id}
            )
            ErrorProcessor.log_error(processed_error)

            # ä¿å­˜å¹¶ç”Ÿæˆé”™è¯¯çŠ¶æ€
            err_content = {
                "role": "system",
                "status_type": "error",
                "message": processed_error.message,
            }
            err_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=err_content,
                is_llm_message=False,
                metadata={
                    "thread_run_id": thread_run_id
                    if "thread_run_id" in locals()
                    else None
                },
            )
            if err_msg_obj:
                yield format_for_yield(err_msg_obj)

            raise

        finally:
            # ä¿å­˜å¹¶ç”Ÿæˆæœ€ç»ˆçš„thread_run_endçŠ¶æ€
            usage = llm_response.usage if hasattr(llm_response, "usage") else None

            end_content = {"status_type": "thread_run_end"}

            end_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=end_content,
                is_llm_message=False,
                metadata={
                    "thread_run_id": thread_run_id
                    if "thread_run_id" in locals()
                    else None
                },
            )
            if end_msg_obj:
                yield format_for_yield(end_msg_obj)

    async def _add_tool_result(
        self,
        thread_id: str,
        tool_call: Dict[str, Any],
        result: ToolResult,
        strategy: Union[XmlAddingStrategy, str] = "assistant_message",
        assistant_message_id: Optional[str] = None,
    ) -> Optional[Dict[str, Any]]:  # è¿”å›žå®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡
        """æ ¹æ®æŒ‡å®šæ ¼å¼å°†å·¥å…·ç»“æžœæ·»åŠ åˆ°å¯¹è¯çº¿ç¨‹ã€‚

        è¯¥æ–¹æ³•æ ¼å¼åŒ–å·¥å…·ç»“æžœå¹¶å°†å…¶æ·»åŠ åˆ°å¯¹è¯åŽ†å²è®°å½•ä¸­ï¼Œ
        ä½¿å…¶åœ¨åŽç»­äº¤äº’ä¸­å¯¹LLMå¯è§ã€‚ç»“æžœå¯ä»¥ä½œä¸º
        åŽŸç”Ÿå·¥å…·æ¶ˆæ¯ï¼ˆOpenAIæ ¼å¼ï¼‰æˆ–å¸¦æœ‰æŒ‡å®šè§’è‰²ï¼ˆç”¨æˆ·æˆ–åŠ©æ‰‹ï¼‰çš„XMLåŒ…è£…å†…å®¹æ·»åŠ ã€‚

        å‚æ•°:
            thread_id: å¯¹è¯çº¿ç¨‹çš„ID
            tool_call: äº§ç”Ÿæ­¤ç»“æžœçš„åŽŸå§‹å·¥å…·è°ƒç”¨
            result: å·¥å…·æ‰§è¡Œçš„ç»“æžœ
            strategy: å¦‚ä½•å°†XMLå·¥å…·ç»“æžœæ·»åŠ åˆ°å¯¹è¯
                     ("user_message", "assistant_message", æˆ– "inline_edit")
            assistant_message_id: ç”Ÿæˆæ­¤å·¥å…·è°ƒç”¨çš„åŠ©æ‰‹æ¶ˆæ¯ID
        """
        try:
            message_obj = None  # åˆå§‹åŒ–message_obj

            # å¦‚æžœæä¾›äº†assistant_message_idï¼Œåˆ™åˆ›å»ºåŒ…å«å®ƒçš„å…ƒæ•°æ®
            metadata = {}
            if assistant_message_id:
                metadata["assistant_message_id"] = assistant_message_id
                logger.debug(f"å°†å·¥å…·ç»“æžœé“¾æŽ¥åˆ°åŠ©æ‰‹æ¶ˆæ¯: {assistant_message_id}")

            # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯åŽŸç”Ÿå‡½æ•°è°ƒç”¨ï¼ˆå…·æœ‰idå­—æ®µï¼‰
            if "id" in tool_call:
                # æ ¹æ®OpenAIè§„èŒƒæ ¼å¼åŒ–ä¸ºé€‚å½“çš„å·¥å…·æ¶ˆæ¯
                function_name = tool_call.get("function_name", "")

                # æ ¼å¼åŒ–å·¥å…·ç»“æžœå†…å®¹ - å·¥å…·è§’è‰²éœ€è¦å­—ç¬¦ä¸²å†…å®¹
                if isinstance(result, str):
                    content = result
                elif hasattr(result, "output"):
                    # å¦‚æžœæ˜¯ToolResultå¯¹è±¡
                    if isinstance(result.output, dict) or isinstance(
                        result.output, list
                    ):
                        # å¦‚æžœè¾“å‡ºå·²ç»æ˜¯dictæˆ–listï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                        content = json.dumps(result.output)
                    else:
                        # å¦åˆ™ä»…ä½¿ç”¨å­—ç¬¦ä¸²è¡¨ç¤º
                        content = str(result.output)
                else:
                    # å›žé€€åˆ°æ•´ä¸ªç»“æžœçš„å­—ç¬¦ä¸²è¡¨ç¤º
                    content = str(result)

                logger.debug(f"æ ¼å¼åŒ–çš„å·¥å…·ç»“æžœå†…å®¹: {content[:100]}...")

                # åˆ›å»ºå…·æœ‰é€‚å½“æ ¼å¼çš„å·¥å…·å“åº”æ¶ˆæ¯
                tool_message = {
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "name": function_name,
                    "content": content,
                }

                logger.debug(
                    f"ä¸ºtool_call_id={tool_call['id']}æ·»åŠ åŽŸç”Ÿå·¥å…·ç»“æžœï¼Œè§’è‰²ä¸ºtool"
                )

                # ä½œä¸ºå·¥å…·æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯åŽ†å²è®°å½•
                # è¿™ä½¿ç»“æžœåœ¨ä¸‹ä¸€è½®å¯¹LLMå¯è§
                message_obj = await self.add_message(
                    thread_id=thread_id,
                    type="tool",  # å·¥å…·å“åº”çš„ç‰¹æ®Šç±»åž‹
                    content=tool_message,
                    is_llm_message=True,
                    metadata=metadata,
                )
                return message_obj  # è¿”å›žå®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡

            # å¯¹äºŽXMLå’Œå…¶ä»–éžåŽŸç”Ÿå·¥å…·ï¼Œä½¿ç”¨æ–°çš„ç»“æž„åŒ–æ ¼å¼
            # æ ¹æ®ç­–ç•¥ç¡®å®šæ¶ˆæ¯è§’è‰²
            result_role = "user" if strategy == "user_message" else "assistant"

            # åˆ›å»ºä¸¤ä¸ªç‰ˆæœ¬çš„ç»“æž„åŒ–ç»“æžœ
            # 1. ç”¨äºŽå‰ç«¯çš„å¯Œç‰ˆæœ¬
            structured_result_for_frontend = self._create_structured_tool_result(
                tool_call, result, parsing_details=None, for_llm=False
            )
            # 2. ç”¨äºŽLLMçš„ç®€æ´ç‰ˆæœ¬
            structured_result_for_llm = self._create_structured_tool_result(
                tool_call, result, parsing_details=None, for_llm=True
            )

            # å°†å…·æœ‰é€‚å½“è§’è‰²çš„æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯åŽ†å²è®°å½•
            # è¿™å…è®¸LLMåœ¨åŽç»­äº¤äº’ä¸­çœ‹åˆ°å·¥å…·ç»“æžœ
            result_message_for_llm = {
                "role": result_role,
                "content": json.dumps(structured_result_for_llm),
            }

            # å°†å¯Œå†…å®¹æ·»åŠ åˆ°å…ƒæ•°æ®ä»¥ä¾›å‰ç«¯ä½¿ç”¨
            if metadata is None:
                metadata = {}
            metadata["frontend_content"] = structured_result_for_frontend

            message_obj = await self._add_message_with_agent_info(
                thread_id=thread_id,
                type="tool",
                content=result_message_for_llm,  # ä¿å­˜LLMå‹å¥½ç‰ˆæœ¬
                is_llm_message=True,
                metadata=metadata,
            )

            # å¦‚æžœæ¶ˆæ¯å·²ä¿å­˜ï¼Œåœ¨è¿”å›žå‰åœ¨å†…å­˜ä¸­ä¸ºå‰ç«¯ä¿®æ”¹å®ƒ
            if message_obj:
                # å‰ç«¯æœŸæœ›åœ¨'content'å­—æ®µä¸­æœ‰å¯Œå†…å®¹ã€‚
                # æ•°æ®åº“åœ¨metadata.frontend_contentä¸­æœ‰å¯Œå†…å®¹ã€‚
                # è®©æˆ‘ä»¬é‡æž„æ¶ˆæ¯ä»¥ä¾›ç”Ÿæˆã€‚
                message_for_yield = message_obj.copy()
                message_for_yield["content"] = structured_result_for_frontend
                return message_for_yield

            return message_obj  # è¿”å›žä¿®æ”¹åŽçš„æ¶ˆæ¯å¯¹è±¡
        except Exception as e:
            logger.error(f"æ·»åŠ å·¥å…·ç»“æžœæ—¶å‡ºé”™: {str(e)}", exc_info=True)
            # å›žé€€åˆ°ç®€å•æ¶ˆæ¯
            try:
                fallback_message = {"role": "user", "content": str(result)}
                message_obj = await self.add_message(
                    thread_id=thread_id,
                    type="tool",
                    content=fallback_message,
                    is_llm_message=True,
                    metadata={"assistant_message_id": assistant_message_id}
                    if assistant_message_id
                    else {},
                )
                return message_obj  # è¿”å›žå®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡
            except Exception as e2:
                logger.error(f"å³ä½¿ä½¿ç”¨å›žé€€æ¶ˆæ¯ä¹Ÿå¤±è´¥: {str(e2)}", exc_info=True)
                return None  # å‡ºé”™æ—¶è¿”å›žNone

    def _create_structured_tool_result(
        self,
        tool_call: Dict[str, Any],
        result: ToolResult,
        parsing_details: Optional[Dict[str, Any]] = None,
        for_llm: bool = False,
    ):
        """åˆ›å»ºä¸Žå·¥å…·æ— å…³ä¸”æä¾›ä¸°å¯Œä¿¡æ¯çš„ç»“æž„åŒ–å·¥å…·ç»“æžœæ ¼å¼ã€‚

        å‚æ•°:
            tool_call: è¢«æ‰§è¡Œçš„åŽŸå§‹å·¥å…·è°ƒç”¨
            result: å·¥å…·æ‰§è¡Œçš„ç»“æžœ
            parsing_details: XMLè°ƒç”¨çš„å¯é€‰è§£æžè¯¦æƒ…
            for_llm: å¦‚æžœä¸ºTrueï¼Œä¸ºLLMä¸Šä¸‹æ–‡åˆ›å»ºç®€æ´ç‰ˆæœ¬ã€‚

        è¿”å›ž:
            åŒ…å«å·¥å…·æ‰§è¡Œä¿¡æ¯çš„ç»“æž„åŒ–å­—å…¸
        """
        # æå–å·¥å…·ä¿¡æ¯
        function_name = tool_call.get("function_name", "unknown")
        arguments = tool_call.get("arguments", {})
        tool_call_id = tool_call.get("id")

        # å¤„ç†è¾“å‡º - å¦‚æžœæ˜¯JSONå­—ç¬¦ä¸²ï¼Œåˆ™è§£æžå›žå¯¹è±¡
        output = result.output if hasattr(result, "output") else str(result)
        if isinstance(output, str):
            try:
                # å°è¯•è§£æžä¸ºJSONä»¥å‘å‰ç«¯æä¾›ç»“æž„åŒ–æ•°æ®
                parsed_output = safe_json_parse(output)
                # å¦‚æžœè§£æžæˆåŠŸä¸”å¾—åˆ°dict/listï¼Œåˆ™ä½¿ç”¨è§£æžç‰ˆæœ¬
                if isinstance(parsed_output, (dict, list)):
                    output = parsed_output
                # å¦åˆ™ä¿ç•™åŽŸå§‹å­—ç¬¦ä¸²
            except Exception:
                # å¦‚æžœè§£æžå¤±è´¥ï¼Œä¿ç•™åŽŸå§‹å­—ç¬¦ä¸²
                pass

        structured_result_v1 = {
            "tool_execution": {
                "function_name": function_name,
                "tool_call_id": tool_call_id,
                "arguments": arguments,
                "result": {
                    "success": result.success if hasattr(result, "success") else True,
                    "output": output,
                    "error": getattr(result, "error", None)
                    if hasattr(result, "error")
                    else None,
                },
            }
        }

        return structured_result_v1

    def _create_tool_context(
        self,
        tool_call: Dict[str, Any],
        tool_index: int,
        assistant_message_id: Optional[str] = None,
    ) -> ToolExecutionContext:
        """åˆ›å»ºåŒ…å«æ˜¾ç¤ºåç§°å’Œè§£æžè¯¦æƒ…çš„å·¥å…·æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚"""
        context = ToolExecutionContext(
            tool_call=tool_call,
            tool_index=tool_index,
            assistant_message_id=assistant_message_id,
        )

        context.function_name = tool_call.get("function_name", "unknown")

        return context

    async def _yield_and_save_tool_started(
        self, context: ToolExecutionContext, thread_id: str, thread_run_id: str
    ) -> Optional[Dict[str, Any]]:
        """æ ¼å¼åŒ–ã€ä¿å­˜å¹¶è¿”å›žå·¥å…·å¼€å§‹çŠ¶æ€æ¶ˆæ¯ã€‚"""
        tool_name = context.function_name
        content = {
            "role": "assistant",
            "status_type": "tool_started",
            "function_name": context.function_name,
            "message": f"å¼€å§‹æ‰§è¡Œ {tool_name}",
            "tool_index": context.tool_index,
            "tool_call_id": context.tool_call.get(
                "id"
            ),  # å¦‚æžœæ˜¯åŽŸç”Ÿçš„ï¼ŒåŒ…å«tool_call ID
        }
        metadata = {"thread_run_id": thread_run_id}
        saved_message_obj = await self.add_message(
            thread_id=thread_id,
            type="status",
            content=content,
            is_llm_message=False,
            metadata=metadata,
        )
        return saved_message_obj  # è¿”å›žå®Œæ•´å¯¹è±¡ï¼ˆå¦‚æžœä¿å­˜å¤±è´¥åˆ™è¿”å›žNoneï¼‰

    async def _yield_and_save_tool_completed(
        self,
        context: ToolExecutionContext,
        tool_message_id: Optional[str],
        thread_id: str,
        thread_run_id: str,
    ) -> Optional[Dict[str, Any]]:
        """æ ¼å¼åŒ–ã€ä¿å­˜å¹¶è¿”å›žå·¥å…·å®Œæˆ/å¤±è´¥çŠ¶æ€æ¶ˆæ¯ã€‚"""
        if not context.result:
            # å¦‚æžœç»“æžœç¼ºå¤±ï¼ˆä¾‹å¦‚æ‰§è¡Œå¤±è´¥ï¼‰ï¼Œå§”æ‰˜ç»™é”™è¯¯ä¿å­˜
            return await self._yield_and_save_tool_error(
                context, thread_id, thread_run_id
            )

        tool_name = context.function_name
        status_type = "tool_completed" if context.result.success else "tool_failed"
        message_text = (
            f"å·¥å…· {tool_name} {'æˆåŠŸå®Œæˆ' if context.result.success else 'å¤±è´¥'}"
        )

        content = {
            "role": "assistant",
            "status_type": status_type,
            "function_name": context.function_name,
            "message": message_text,
            "tool_index": context.tool_index,
            "tool_call_id": context.tool_call.get("id"),
        }
        metadata = {"thread_run_id": thread_run_id}
        # å¦‚æžœå¯ç”¨ä¸”æˆåŠŸï¼Œå°†*å®žé™…*å·¥å…·ç»“æžœæ¶ˆæ¯IDæ·»åŠ åˆ°å…ƒæ•°æ®
        if context.result.success and tool_message_id:
            metadata["linked_tool_result_message_id"] = tool_message_id

        # <<< æ·»åŠ ï¼šå¦‚æžœè¿™æ˜¯ç»ˆæ­¢å·¥å…·ï¼Œåˆ™å‘å‡ºä¿¡å· >>>
        if context.function_name in ["ask", "complete"]:
            metadata["agent_should_terminate"] = "true"
            logger.debug(f"ä½¿ç”¨ç»ˆæ­¢ä¿¡å·æ ‡è®°å·¥å…·çŠ¶æ€ '{context.function_name}'ã€‚")
        # <<< ç»“æŸæ·»åŠ  >>>

        saved_message_obj = await self.add_message(
            thread_id=thread_id,
            type="status",
            content=content,
            is_llm_message=False,
            metadata=metadata,
        )
        return saved_message_obj

    async def _yield_and_save_tool_error(
        self, context: ToolExecutionContext, thread_id: str, thread_run_id: str
    ) -> Optional[Dict[str, Any]]:
        """æ ¼å¼åŒ–ã€ä¿å­˜å¹¶è¿”å›žå·¥å…·é”™è¯¯çŠ¶æ€æ¶ˆæ¯ã€‚"""
        error_msg = str(context.error) if context.error else "å·¥å…·æ‰§è¡ŒæœŸé—´æœªçŸ¥é”™è¯¯"
        tool_name = context.function_name
        content = {
            "role": "assistant",
            "status_type": "tool_error",
            "function_name": context.function_name,
            "message": f"æ‰§è¡Œå·¥å…· {tool_name} æ—¶å‡ºé”™: {error_msg}",
            "tool_index": context.tool_index,
            "tool_call_id": context.tool_call.get("id"),
        }
        metadata = {"thread_run_id": thread_run_id}
        # ä½¿ç”¨is_llm_message=Falseä¿å­˜çŠ¶æ€æ¶ˆæ¯
        saved_message_obj = await self.add_message(
            thread_id=thread_id,
            type="status",
            content=content,
            is_llm_message=False,
            metadata=metadata,
        )
        return saved_message_obj

    def _format_assistant_message_content(self, content_blocks: List) -> Dict:
        """æ ¼å¼åŒ–assistantæ¶ˆæ¯å†…å®¹ï¼Œå…¼å®¹TextBlockå’ŒToolUseBlockã€‚

        Args:
            content_blocks: Claude Codeè¿”å›žçš„content blocksåˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–åŽçš„æ¶ˆæ¯å†…å®¹å­—å…¸
        """
        text_parts = []
        tool_calls = []

        for block in content_blocks:
            block_type = type(block).__name__
            if block_type == "TextBlock":
                text_parts.append(block.text)
            elif block_type == "ToolUseBlock":
                tool_calls.append(
                    {"id": block.id, "name": block.name, "input": block.input}
                )

        result = {"role": "assistant", "content": "".join(text_parts)}

        if tool_calls:
            result["tool_calls"] = tool_calls

        return result

    def _serialize_claude_code_response(
        self, claude_response, usage_data: Dict
    ) -> Dict:
        """åºåˆ—åŒ–Claude Codeå“åº”å¯¹è±¡ç”¨äºŽä¿å­˜llm_response_endã€‚

        Args:
            claude_response: Claude Codeçš„ResultMessageæˆ–å…¶ä»–å“åº”å¯¹è±¡
            usage_data: æ”¶é›†çš„usageæ•°æ®

        Returns:
            åºåˆ—åŒ–åŽçš„å“åº”å†…å®¹
        """
        result = {
            "model": getattr(claude_response, "model", "kimi-for-coding"),
            "usage": usage_data if usage_data else {},
        }

        # å¦‚æžœæ˜¯ResultMessageï¼Œæå–æ›´å¤šä¿¡æ¯
        if hasattr(claude_response, "total_cost_usd"):
            result["total_cost_usd"] = claude_response.total_cost_usd
        if hasattr(claude_response, "num_turns"):
            result["num_turns"] = claude_response.num_turns

        return result
