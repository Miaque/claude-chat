import asyncio
import json
import uuid
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import (
    Any,
    AsyncGenerator,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    Union,
)
from zoneinfo import ZoneInfo

from loguru import logger

from core.error_processor import ErrorProcessor
from core.tool import ToolResult
from core.utils.json_helpers import (
    ensure_dict,
    format_for_yield,
    safe_json_parse,
    to_json_string,
)

# XMLç»“æœæ·»åŠ ç­–ç•¥çš„ç±»å‹åˆ«å
XmlAddingStrategy = Literal["user_message", "assistant_message", "inline_edit"]


@dataclass
class ToolExecutionContext:
    """å·¥å…·æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ŒåŒ…å«è°ƒç”¨è¯¦æƒ…ã€ç»“æœå’Œæ˜¾ç¤ºä¿¡æ¯ã€‚"""

    tool_call: Dict[str, Any]
    tool_index: int
    result: Optional[ToolResult] = None
    function_name: Optional[str] = None
    xml_tag_name: Optional[str] = None
    error: Optional[Exception] = None
    assistant_message_id: Optional[str] = None
    parsing_details: Optional[Dict[str, Any]] = None


@dataclass
class ProcessorConfig:
    """
    å“åº”å¤„ç†å’Œå·¥å…·æ‰§è¡Œçš„é…ç½®ã€‚

    è¯¥ç±»æ§åˆ¶LLMå“åº”çš„å¤„ç†æ–¹å¼ï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨çš„æ£€æµ‹ã€æ‰§è¡Œå’Œç»“æœå¤„ç†ã€‚

    å±æ€§:
        execute_on_stream: å¯¹äºæµå¼å“åº”ï¼Œæ˜¯å³æ—¶æ‰§è¡Œå·¥å…·è¿˜æ˜¯åœ¨ç»“æŸæ—¶æ‰§è¡Œ
    """

    execute_on_stream: bool = False


class ResponseProcessor:
    """å¤„ç†LLMå“åº”ï¼Œæå–å¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨ã€‚"""

    def __init__(
        self,
        add_message_callback: Callable,
        agent_config: Optional[dict] = None,
    ):
        """åˆå§‹åŒ–ResponseProcessorã€‚

        å‚æ•°:
            add_message_callback: å‘çº¿ç¨‹æ·»åŠ æ¶ˆæ¯çš„å›è°ƒå‡½æ•°ã€‚
                å¿…é¡»è¿”å›å®Œæ•´ä¿å­˜çš„æ¶ˆæ¯å¯¹è±¡(dict)æˆ–Noneã€‚
            agent_config: å¯é€‰çš„ä»£ç†é…ç½®ï¼ŒåŒ…å«ç‰ˆæœ¬ä¿¡æ¯
        """
        self.add_message = add_message_callback

        self.agent_config = agent_config

    async def _yield_message(
        self, message_obj: Optional[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        """è¾…åŠ©æ–¹æ³•ï¼šä»¥é€‚å½“çš„æ ¼å¼ç”Ÿæˆæ¶ˆæ¯ã€‚

        ç¡®ä¿å†…å®¹å’Œå…ƒæ•°æ®ä¸ºJSONå­—ç¬¦ä¸²ï¼Œä»¥ä¿è¯å®¢æˆ·ç«¯å…¼å®¹æ€§ã€‚
        """
        if message_obj:
            return format_for_yield(message_obj)
        return None

    def _serialize_model_response(self, model_response) -> Dict[str, Any]:
        """å°†ModelResponseå¯¹è±¡è½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„å­—å…¸ã€‚

        å‚æ•°:
            model_response: ModelResponseå¯¹è±¡

        è¿”å›:
            ModelResponseçš„å­—å…¸è¡¨ç¤º
        """
        try:
            # å°è¯•ä½¿ç”¨model_dumpæ–¹æ³•ï¼ˆPydantic v2ï¼‰
            if hasattr(model_response, "model_dump"):
                return model_response.model_dump()

            # å°è¯•ä½¿ç”¨dictæ–¹æ³•ï¼ˆPydantic v1ï¼‰
            elif hasattr(model_response, "dict"):
                return model_response.dict()

            # å›é€€ï¼šæ‰‹åŠ¨æå–å¸¸è§å±æ€§
            else:
                result = {}

                # å¸¸è§çš„LiteLLM ModelResponseå±æ€§
                for attr in [
                    "id",
                    "object",
                    "created",
                    "model",
                    "choices",
                    "usage",
                    "system_fingerprint",
                ]:
                    if hasattr(model_response, attr):
                        value = getattr(model_response, attr)
                        # é€’å½’å¤„ç†åµŒå¥—å¯¹è±¡
                        if hasattr(value, "model_dump"):
                            result[attr] = value.model_dump()
                        elif hasattr(value, "dict"):
                            result[attr] = value.dict()
                        elif isinstance(value, list):
                            result[attr] = [
                                item.model_dump()
                                if hasattr(item, "model_dump")
                                else item.dict()
                                if hasattr(item, "dict")
                                else item
                                for item in value
                            ]
                        else:
                            result[attr] = value

                return result

        except Exception as e:
            logger.warning(
                f"åºåˆ—åŒ–ModelResponseå¤±è´¥: {str(e)}, å›é€€åˆ°å­—ç¬¦ä¸²è¡¨ç¤º"
            )
            # æœ€ç»ˆå›é€€ï¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            return {"raw_response": str(model_response), "serialization_error": str(e)}

    async def _add_message_with_agent_info(
        self,
        thread_id: str,
        type: str,
        content: Union[Dict[str, Any], List[Any], str],
        is_llm_message: bool = False,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """è¾…åŠ©æ–¹æ³•ï¼šåœ¨å¯ç”¨æ—¶æ·»åŠ åŒ…å«ä»£ç†ç‰ˆæœ¬ä¿¡æ¯çš„æ¶ˆæ¯ã€‚"""
        agent_id = None
        agent_version_id = None

        if self.agent_config:
            agent_id = self.agent_config.get("agent_id")
            agent_version_id = self.agent_config.get("current_version_id")

        return await self.add_message(
            thread_id=thread_id,
            type=type,
            content=content,
            is_llm_message=is_llm_message,
            metadata=metadata,
            agent_id=agent_id,
            agent_version_id=agent_version_id,
        )

    async def process_streaming_response(
        self,
        llm_response: AsyncGenerator,
        thread_id: str,
        prompt_messages: List[Dict[str, Any]],
        llm_model: str,
        config: ProcessorConfig = ProcessorConfig(),
        can_auto_continue: bool = False,
        auto_continue_count: int = 0,
        continuous_state: Optional[Dict[str, Any]] = None,
        estimated_total_tokens: Optional[int] = None,
        cancellation_event: Optional[asyncio.Event] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """å¤„ç†æµå¼LLMå“åº”ï¼Œå¤„ç†å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œã€‚

        å‚æ•°:
            llm_response: LLMçš„æµå¼å“åº”
            thread_id: å¯¹è¯çº¿ç¨‹çš„ID
            prompt_messages: å‘é€ç»™LLMçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæç¤ºï¼‰
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
            config: è§£æå’Œæ‰§è¡Œçš„é…ç½®
            can_auto_continue: æ˜¯å¦å¯ç”¨è‡ªåŠ¨ç»§ç»­
            auto_continue_count: è‡ªåŠ¨ç»§ç»­çš„å¾ªç¯æ¬¡æ•°
            continuous_state: å¯¹è¯çš„å…ˆå‰çŠ¶æ€

        ç”Ÿæˆ:
            å®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡ï¼ŒåŒ¹é…æ•°æ®åº“æ¨¡å¼ï¼Œé™¤äº†å†…å®¹å—ã€‚
        """
        logger.info(f"å¼€å§‹ä¸ºçº¿ç¨‹ {thread_id} å¤„ç†æµå¼å“åº”")

        # å¦‚æœæœªæä¾›ï¼Œåˆå§‹åŒ–å–æ¶ˆäº‹ä»¶
        if cancellation_event is None:
            cancellation_event = asyncio.Event()

        # å¦‚æœæä¾›äº†continuous_stateåˆ™ä»ä¸­åˆå§‹åŒ–ï¼ˆç”¨äºè‡ªåŠ¨ç»§ç»­ï¼‰
        continuous_state = continuous_state or {}
        accumulated_content = continuous_state.get("accumulated_content", "")
        current_xml_content = accumulated_content  # å¦‚æœè‡ªåŠ¨ç»§ç»­åˆ™ç­‰äºaccumulated_contentï¼Œå¦åˆ™ä¸ºç©º
        finish_reason = None
        should_auto_continue = False
        last_assistant_message_object = (
            None  # å­˜å‚¨æœ€ç»ˆä¿å­˜çš„åŠ©æ‰‹æ¶ˆæ¯å¯¹è±¡
        )
        has_printed_thinking_prefix = (
            False  # ä»…æ‰“å°ä¸€æ¬¡æ€è€ƒå‰ç¼€çš„æ ‡å¿—
        )
        agent_should_terminate = (
            False  # è·Ÿè¸ªæ˜¯å¦å·²æ‰§è¡Œç»ˆæ­¢å·¥å…·çš„æ ‡å¿—
        )
        complete_native_tool_calls = []  # æå‰åˆå§‹åŒ–ï¼Œä¾›assistant_response_endä½¿ç”¨

        # å­˜å‚¨æ¥æ”¶åˆ°çš„å®Œæ•´LiteLLMå“åº”å¯¹è±¡
        final_llm_response = None
        first_chunk_time = None
        last_chunk_time = None
        llm_response_end_saved = False

        # é‡ç”¨thread_run_idç”¨äºè‡ªåŠ¨ç»§ç»­æˆ–åˆ›å»ºæ–°çš„
        thread_run_id = continuous_state.get("thread_run_id") or str(uuid.uuid4())
        continuous_state["thread_run_id"] = thread_run_id

        # å…³é”®ï¼šä¸ºæœ¬æ¬¡ç‰¹å®šçš„LLMè°ƒç”¨ç”Ÿæˆå”¯ä¸€IDï¼ˆä¸æ˜¯æ¯ä¸ªçº¿ç¨‹è¿è¡Œï¼‰
        llm_response_id = str(uuid.uuid4())
        logger.info(
            f"ğŸ”µ LLM è°ƒç”¨ #{auto_continue_count + 1} å¼€å§‹ - llm_response_id: {llm_response_id}"
        )

        try:
            # --- ä¿å­˜å¹¶ç”Ÿæˆå¼€å§‹äº‹ä»¶ ---
            if auto_continue_count == 0:
                start_content = {
                    "status_type": "thread_run_start",
                    "thread_run_id": thread_run_id,
                }
                start_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=start_content,
                    is_llm_message=False,
                    metadata={"thread_run_id": thread_run_id},
                )
                if start_msg_obj:
                    yield format_for_yield(start_msg_obj)

            llm_start_content = {
                "llm_response_id": llm_response_id,
                "auto_continue_count": auto_continue_count,
                "model": llm_model,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            llm_start_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="llm_response_start",
                content=llm_start_content,
                is_llm_message=False,
                metadata={
                    "thread_run_id": thread_run_id,
                    "llm_response_id": llm_response_id,
                },
            )
            if llm_start_msg_obj:
                yield format_for_yield(llm_start_msg_obj)
                logger.info(
                    f"âœ… å·²ä¿å­˜ç¬¬ #{auto_continue_count + 1} æ¬¡è°ƒç”¨çš„ llm_response_start"
                )
            # --- ç»“æŸå¼€å§‹äº‹ä»¶ ---

            __sequence = continuous_state.get(
                "sequence", 0
            )  # ä»ä¸Šä¸€ä¸ªè‡ªåŠ¨ç»§ç»­å¾ªç¯ä¸­è·å–åºåˆ—

            chunk_count = 0
            async for chunk in llm_response:
                # å¤„ç†æ¯ä¸ªå—ä¹‹å‰æ£€æŸ¥å–æ¶ˆ
                if cancellation_event.is_set():
                    logger.info(f"çº¿ç¨‹ {thread_id} æ”¶åˆ°å–æ¶ˆä¿¡å· - åœæ­¢ LLM æµå¤„ç†")
                    finish_reason = "cancelled"
                    break

                chunk_count += 1

                # è·Ÿè¸ªæ—¶é—´
                current_time = datetime.now().timestamp()
                if first_chunk_time is None:
                    first_chunk_time = current_time
                last_chunk_time = current_time

                # å®šæœŸè®°å½•å—ä¿¡æ¯ä»¥ç”¨äºè°ƒè¯•
                if (
                    chunk_count == 1
                    or (chunk_count % 1000 == 0)
                    or hasattr(chunk, "usage")
                ):
                    logger.debug(f"å¤„ç†å— #{chunk_count}, ç±»å‹={type(chunk).__name__}")

                ## å½“æˆ‘ä»¬è·å¾—ä½¿ç”¨æ•°æ®æ—¶ï¼Œå­˜å‚¨å®Œæ•´çš„LiteLLMå“åº”å—
                if (
                    hasattr(chunk, "usage")
                    and chunk.usage
                    and final_llm_response is None
                ):
                    logger.info(
                        "ğŸ” å­˜å‚¨æ¥æ”¶åˆ°çš„å®Œæ•´ LiteLLM å“åº”å—"
                    )
                    final_llm_response = chunk  # æŒ‰åŸæ ·å­˜å‚¨æ•´ä¸ªå—å¯¹è±¡
                    logger.info(
                        f"ğŸ” å­˜å‚¨çš„æ¨¡å‹: {getattr(chunk, 'model', 'NO_MODEL')}"
                    )
                    logger.info(f"ğŸ” å­˜å‚¨çš„ä½¿ç”¨é‡: {chunk.usage}")
                    logger.info(f"ğŸ” å­˜å‚¨çš„å“åº”ç±»å‹: {type(chunk)}")

                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and hasattr(chunk.choices[0], "finish_reason")
                    and chunk.choices[0].finish_reason
                ):
                    finish_reason = chunk.choices[0].finish_reason
                    logger.debug(f"æ£€æµ‹åˆ° finish_reasonï¼š{finish_reason}")

                if hasattr(chunk, "choices") and chunk.choices:
                    delta = (
                        chunk.choices[0].delta
                        if hasattr(chunk.choices[0], "delta")
                        else None
                    )

                    # æ£€æŸ¥å¹¶è®°å½•Anthropicçš„æ€è€ƒå†…å®¹
                    if (
                        delta
                        and hasattr(delta, "reasoning_content")
                        and delta.reasoning_content
                    ):
                        if not has_printed_thinking_prefix:
                            # print("[THINKING]: ", end='', flush=True)
                            has_printed_thinking_prefix = True
                        # print(delta.reasoning_content, end='', flush=True)
                        # å°†æ¨ç†å†…å®¹è¿½åŠ åˆ°ä¸»å†…å®¹ä»¥ä¿å­˜åœ¨æœ€ç»ˆæ¶ˆæ¯ä¸­
                        reasoning_content = delta.reasoning_content
                        # logger.debug(f"å¤„ç† reasoning_content: ç±»å‹={type(reasoning_content)}, å€¼={reasoning_content}")
                        if isinstance(reasoning_content, list):
                            reasoning_content = "".join(
                                str(item) for item in reasoning_content
                            )
                        # logger.debug(f"å³å°†è¿æ¥ reasoning_content (ç±»å‹={type(reasoning_content)}) åˆ° accumulated_content (ç±»å‹={type(accumulated_content)})")
                        accumulated_content += reasoning_content

                    # å¤„ç†å†…å®¹å—
                    if delta and hasattr(delta, "content") and delta.content:
                        chunk_content = delta.content
                        # logger.debug(f"å¤„ç† chunk_content: ç±»å‹={type(chunk_content)}, å€¼={chunk_content}")
                        if isinstance(chunk_content, list):
                            chunk_content = "".join(str(item) for item in chunk_content)
                        # print(chunk_content, end='', flush=True)
                        # logger.debug(f"å³å°†è¿æ¥ chunk_content (ç±»å‹={type(chunk_content)}) åˆ° accumulated_content (ç±»å‹={type(accumulated_content)})")
                        accumulated_content += chunk_content
                        # logger.debug(f"å³å°†è¿æ¥ chunk_content (ç±»å‹={type(chunk_content)}) åˆ° current_xml_content (ç±»å‹={type(current_xml_content)})")
                        current_xml_content += chunk_content

                        # ä»…ç”Ÿæˆå†…å®¹å—ï¼ˆä¸ä¿å­˜ï¼‰
                        now_chunk = datetime.now()
                        yield {
                            "sequence": __sequence,
                            "message_id": None,
                            "thread_id": thread_id,
                            "type": "assistant",
                            "is_llm_message": True,
                            "content": to_json_string(
                                {"role": "assistant", "content": chunk_content}
                            ),
                            "metadata": to_json_string(
                                {
                                    "stream_status": "chunk",
                                    "thread_run_id": thread_run_id,
                                }
                            ),
                            "created_at": now_chunk,
                            "updated_at": now_chunk,
                        }
                        __sequence += 1

            logger.info(f"æµå¤„ç†å®Œæˆã€‚æ€»å—æ•°ï¼š{chunk_count}")

            # å¦‚æœæœ‰æ—¶é—´æ•°æ®ï¼Œè®¡ç®—å“åº”æ—¶é—´
            response_ms = None
            if first_chunk_time and last_chunk_time:
                response_ms = (last_chunk_time - first_chunk_time) * 1000

            # è®°å½•æˆ‘ä»¬æ•è·çš„å†…å®¹
            if final_llm_response:
                logger.info("âœ… å·²æ•è·å®Œæ•´çš„ LiteLLM å“åº”å¯¹è±¡")
                logger.info(
                    f"ğŸ” å“åº”æ¨¡å‹: {getattr(final_llm_response, 'model', 'NO_MODEL')}"
                )
                logger.info(
                    f"ğŸ” å“åº”ä½¿ç”¨é‡: {getattr(final_llm_response, 'usage', 'NO_USAGE')}"
                )
            else:
                logger.warning(
                    "âš ï¸ æœªä»æµå¼å—ä¸­æ•è·å®Œæ•´çš„ LiteLLM å“åº”"
                )

            should_auto_continue = can_auto_continue and finish_reason == "length"

            # å¦‚æœç”¨æˆ·åœæ­¢ï¼ˆå–æ¶ˆï¼‰ï¼Œåˆ™ä¸ä¿å­˜éƒ¨åˆ†å“åº”
            # ä½†å¯¹äºå…¶ä»–æå‰åœæ­¢ï¼ˆå¦‚è¾¾åˆ°XMLé™åˆ¶ï¼‰è¦ä¿å­˜
            if (
                accumulated_content
                and not should_auto_continue
                and finish_reason != "cancelled"
            ):
                message_data = {  # è¦ä¿å­˜åœ¨'content'ä¸­çš„å­—å…¸
                    "role": "assistant",
                    "content": accumulated_content,
                    "tool_calls": complete_native_tool_calls or None,
                }

                last_assistant_message_object = await self._add_message_with_agent_info(
                    thread_id=thread_id,
                    type="assistant",
                    content=message_data,
                    is_llm_message=True,
                    metadata={"thread_run_id": thread_run_id},
                )

                if last_assistant_message_object:
                    # ç”Ÿæˆå®Œæ•´ä¿å­˜çš„å¯¹è±¡ï¼Œä»…ä¸ºç”Ÿæˆæ·»åŠ stream_statuså…ƒæ•°æ®
                    yield_metadata = ensure_dict(
                        last_assistant_message_object.get("metadata"), {}
                    )
                    yield_metadata["stream_status"] = "complete"
                    # æ ¼å¼åŒ–æ¶ˆæ¯ä»¥ä¾›ç”Ÿæˆ
                    yield_message = last_assistant_message_object.copy()
                    yield_message["metadata"] = yield_metadata
                    yield format_for_yield(yield_message)
                else:
                    logger.error(
                        f"ä¸ºçº¿ç¨‹ {thread_id} ä¿å­˜æœ€ç»ˆåŠ©æ‰‹æ¶ˆæ¯å¤±è´¥"
                    )
                    # ä¿å­˜å¹¶ç”Ÿæˆé”™è¯¯çŠ¶æ€
                    err_content = {
                        "role": "system",
                        "status_type": "error",
                        "message": "ä¿å­˜æœ€ç»ˆåŠ©æ‰‹æ¶ˆæ¯å¤±è´¥",
                    }
                    err_msg_obj = await self.add_message(
                        thread_id=thread_id,
                        type="status",
                        content=err_content,
                        is_llm_message=False,
                        metadata={"thread_run_id": thread_run_id},
                    )
                    if err_msg_obj:
                        yield format_for_yield(err_msg_obj)

            # --- æœ€ç»ˆå®ŒæˆçŠ¶æ€ ---
            if finish_reason:
                finish_content = {
                    "status_type": "finish",
                    "finish_reason": finish_reason,
                }
                finish_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=finish_content,
                    is_llm_message=False,
                    metadata={"thread_run_id": thread_run_id},
                )
                if finish_msg_obj:
                    yield format_for_yield(finish_msg_obj)

            # æ£€æŸ¥åœ¨å¤„ç†å¾…å¤„ç†å·¥å…·åä»£ç†æ˜¯å¦åº”è¯¥ç»ˆæ­¢
            if agent_should_terminate:
                logger.debug(
                    "æ‰§è¡Œask/completeå·¥å…·åè¯·æ±‚ä»£ç†ç»ˆæ­¢ã€‚åœæ­¢è¿›ä¸€æ­¥å¤„ç†ã€‚"
                )

                # è®¾ç½®finish_reasonä»¥æŒ‡ç¤ºç»ˆæ­¢
                finish_reason = "agent_terminated"

                # ä¿å­˜å¹¶ç”Ÿæˆç»ˆæ­¢çŠ¶æ€
                finish_content = {
                    "status_type": "finish",
                    "finish_reason": "agent_terminated",
                }
                finish_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=finish_content,
                    is_llm_message=False,
                    metadata={"thread_run_id": thread_run_id},
                )
                if finish_msg_obj:
                    yield format_for_yield(finish_msg_obj)

                # åœ¨ç»ˆæ­¢å‰ä¿å­˜llm_response_end
                if last_assistant_message_object:
                    try:
                        # ä½¿ç”¨æ¥æ”¶åˆ°çš„å®Œæ•´LiteLLMå“åº”å¯¹è±¡
                        if final_llm_response:
                            logger.info(
                                "âœ… åœ¨ç»ˆæ­¢å‰ä½¿ç”¨å®Œæ•´çš„LiteLLMå“åº”è¿›è¡Œllm_response_end"
                            )
                            # æŒ‰åŸæ ·åºåˆ—åŒ–å®Œæ•´çš„å“åº”å¯¹è±¡
                            llm_end_content = self._serialize_model_response(
                                final_llm_response
                            )

                            # æ·»åŠ æµå¼æ ‡å¿—å’Œå“åº”æ—¶é—´ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            llm_end_content["streaming"] = True
                            if response_ms:
                                llm_end_content["response_ms"] = response_ms

                            # å¯¹äºæµå¼å“åº”ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ„å»ºchoices
                            # å› ä¸ºæµå¼å—æ²¡æœ‰å®Œæ•´çš„æ¶ˆæ¯ç»“æ„
                            llm_end_content["choices"] = [
                                {
                                    "finish_reason": finish_reason or "stop",
                                    "index": 0,
                                    "message": {
                                        "role": "assistant",
                                        "content": accumulated_content,
                                        "tool_calls": complete_native_tool_calls
                                        or None,
                                    },
                                }
                            ]
                            llm_end_content["llm_response_id"] = llm_response_id
                        else:
                            logger.warning(
                                "âš ï¸ æ²¡æœ‰å¯ç”¨çš„å®Œæ•´LiteLLMå“åº”ï¼Œè·³è¿‡llm_response_end"
                            )
                            llm_end_content = None

                        # åªæœ‰åœ¨æˆ‘ä»¬æœ‰å†…å®¹æ—¶æ‰ä¿å­˜
                        if llm_end_content:
                            llm_end_msg_obj = await self.add_message(
                                thread_id=thread_id,
                                type="llm_response_end",
                                content=llm_end_content,
                                is_llm_message=False,
                                metadata={
                                    "thread_run_id": thread_run_id,
                                    "llm_response_id": llm_response_id,
                                },
                            )
                            llm_response_end_saved = True
                            # ç”Ÿæˆåˆ°æµä¸­ä»¥å®æ—¶æ›´æ–°ä¸Šä¸‹æ–‡ä½¿ç”¨é‡
                            if llm_end_msg_obj:
                                yield format_for_yield(llm_end_msg_obj)
                        logger.info(
                            f"âœ… åœ¨ç¬¬ #{auto_continue_count + 1} æ¬¡è°ƒç”¨å‰ç»ˆæ­¢æ—¶å·²ä¿å­˜llm_response_end"
                        )
                    except Exception as e:
                        logger.error(
                            f"åœ¨ç»ˆæ­¢å‰ä¿å­˜llm_response_endæ—¶å‡ºé”™: {str(e)}"
                        )

                # è·³è¿‡æ‰€æœ‰å‰©ä½™å¤„ç†å¹¶è½¬åˆ°finallyå—
                return

            # --- ä¿å­˜å¹¶ç”Ÿæˆllm_response_end ---
            # ä»…åœ¨éè‡ªåŠ¨ç»§ç»­æ—¶ä¿å­˜llm_response_endï¼ˆå“åº”å®é™…å®Œæˆï¼‰
            if not should_auto_continue:
                if last_assistant_message_object:
                    try:
                        # ä½¿ç”¨æ¥æ”¶åˆ°çš„å®Œæ•´LiteLLMå“åº”å¯¹è±¡
                        if final_llm_response:
                            logger.info(
                                "âœ… åœ¨æ­£å¸¸å®Œæˆæ—¶ä½¿ç”¨å®Œæ•´çš„LiteLLMå“åº”è¿›è¡Œllm_response_end"
                            )

                            # è®°å½•å®Œæ•´çš„å“åº”å¯¹è±¡ä»¥ç”¨äºè°ƒè¯•
                            logger.info(
                                f"ğŸ” å®Œæ•´å“åº”å¯¹è±¡: {final_llm_response}"
                            )
                            logger.info(
                                f"ğŸ” å“åº”å¯¹è±¡ç±»å‹: {type(final_llm_response)}"
                            )
                            logger.info(
                                f"ğŸ” å“åº”å¯¹è±¡å­—å…¸: {final_llm_response.__dict__ if hasattr(final_llm_response, '__dict__') else 'NO_DICT'}"
                            )

                            # æŒ‰åŸæ ·åºåˆ—åŒ–å®Œæ•´çš„å“åº”å¯¹è±¡
                            llm_end_content = self._serialize_model_response(
                                final_llm_response
                            )
                            logger.info(f"ğŸ” åºåˆ—åŒ–å†…å®¹: {llm_end_content}")

                            # æ·»åŠ æµå¼æ ‡å¿—å’Œå“åº”æ—¶é—´ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            llm_end_content["streaming"] = True
                            if response_ms:
                                llm_end_content["response_ms"] = response_ms

                            # å¯¹äºæµå¼å“åº”ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ„å»ºchoices
                            # å› ä¸ºæµå¼å—æ²¡æœ‰å®Œæ•´çš„æ¶ˆæ¯ç»“æ„
                            llm_end_content["choices"] = [
                                {
                                    "finish_reason": finish_reason or "stop",
                                    "index": 0,
                                    "message": {
                                        "role": "assistant",
                                        "content": accumulated_content,
                                        "tool_calls": complete_native_tool_calls
                                        or None,
                                    },
                                }
                            ]
                            llm_end_content["llm_response_id"] = llm_response_id

                            # è°ƒè¯•ï¼šè®°å½•å®é™…å“åº”ä½¿ç”¨é‡
                            logger.info(
                                f"ğŸ” å“åº”å¤„ç†å™¨å®Œæˆä½¿ç”¨é‡ (æ­£å¸¸): {llm_end_content.get('usage', 'NO_USAGE')}"
                            )
                            logger.info(f"ğŸ” æœ€ç»ˆLLMç»“æŸå†…å®¹: {llm_end_content}")

                            llm_end_msg_obj = await self.add_message(
                                thread_id=thread_id,
                                type="llm_response_end",
                                content=llm_end_content,
                                is_llm_message=False,
                                metadata={
                                    "thread_run_id": thread_run_id,
                                    "llm_response_id": llm_response_id,
                                },
                            )
                            llm_response_end_saved = True
                            # ç”Ÿæˆåˆ°æµä¸­ä»¥å®æ—¶æ›´æ–°ä¸Šä¸‹æ–‡ä½¿ç”¨é‡
                            if llm_end_msg_obj:
                                yield format_for_yield(llm_end_msg_obj)
                        else:
                            logger.warning(
                                "âš ï¸ æ²¡æœ‰å¯ç”¨çš„å®Œæ•´LiteLLMå“åº”ï¼Œè·³è¿‡llm_response_end"
                            )
                        logger.info(
                            f"âœ… åœ¨ç¬¬ #{auto_continue_count + 1} æ¬¡è°ƒç”¨æ­£å¸¸å®Œæˆæ—¶å·²ä¿å­˜llm_response_end"
                        )
                    except Exception as e:
                        logger.error(f"ä¿å­˜llm_response_endæ—¶å‡ºé”™: {str(e)}")

        except Exception as e:
            # ä½¿ç”¨ErrorProcessorè¿›è¡Œä¸€è‡´çš„é”™è¯¯å¤„ç†
            processed_error = ErrorProcessor.process_system_error(
                e, context={"thread_id": thread_id}
            )
            ErrorProcessor.log_error(processed_error)

            # ä¿å­˜å¹¶ç”Ÿæˆé”™è¯¯çŠ¶æ€æ¶ˆæ¯
            err_content = {
                "role": "system",
                "status_type": "error",
                "message": processed_error.message,
            }
            err_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=err_content,
                is_llm_message=False,
                metadata={
                    "thread_run_id": thread_run_id
                    if "thread_run_id" in locals()
                    else None
                },
            )
            if err_msg_obj:
                yield format_for_yield(err_msg_obj)
            raise

        finally:
            # é‡è¦ï¼šfinallyå—å³ä½¿åœ¨æµåœæ­¢æ—¶ä¹Ÿä¼šè¿è¡Œï¼ˆGeneratorExitï¼‰
            # æˆ‘ä»¬ç»ä¸èƒ½åœ¨è¿™é‡Œç”Ÿæˆ - åªéœ€é™é»˜ä¿å­˜åˆ°æ•°æ®åº“ä»¥ç”¨äºè®¡è´¹/ä½¿ç”¨è·Ÿè¸ª

            # é˜¶æ®µ3ï¼šèµ„æºæ¸…ç† - å–æ¶ˆå¾…å¤„ç†ä»»åŠ¡å¹¶å…³é—­ç”Ÿæˆå™¨
            try:
                # å¦‚æœLLMå“åº”ç”Ÿæˆå™¨æ”¯æŒaclose()ï¼Œåˆ™å°è¯•å…³é—­å®ƒ
                # è¿™æœ‰åŠ©äºé˜»æ­¢åº•å±‚HTTPè¿æ¥ç»§ç»­
                if hasattr(llm_response, "aclose"):
                    try:
                        await llm_response.aclose()
                        logger.debug(
                            f"å·²å…³é—­çº¿ç¨‹ {thread_id} çš„LLMå“åº”ç”Ÿæˆå™¨"
                        )
                    except Exception as close_err:
                        logger.debug(
                            f"å…³é—­LLMå“åº”ç”Ÿæˆå™¨æ—¶å‡ºé”™ï¼ˆå¯èƒ½ä¸æ”¯æŒacloseï¼‰: {close_err}"
                        )
                elif hasattr(llm_response, "close") and callable(getattr(llm_response, "close")):
                    try:
                        llm_response.close()  # type: ignore
                        logger.debug(
                            f"å·²å…³é—­çº¿ç¨‹ {thread_id} çš„LLMå“åº”ç”Ÿæˆå™¨ï¼ˆåŒæ­¥å…³é—­ï¼‰"
                        )
                    except Exception as close_err:
                        logger.debug(
                            f"å…³é—­LLMå“åº”ç”Ÿæˆå™¨æ—¶å‡ºé”™ï¼ˆåŒæ­¥ï¼‰: {close_err}"
                        )
            except Exception as cleanup_err:
                logger.warning(f"èµ„æºæ¸…ç†æœŸé—´å‡ºé”™: {cleanup_err}")

            if not llm_response_end_saved and last_assistant_message_object:
                try:
                    logger.info(
                        f"ğŸ’° é˜²å¼¹è®¡è´¹ï¼šåœ¨finallyå—ä¸­ä¸ºç¬¬ #{auto_continue_count + 1} æ¬¡è°ƒç”¨ä¿å­˜llm_response_end"
                    )
                    if final_llm_response:
                        logger.info("ğŸ’° ä½¿ç”¨LLMå“åº”çš„ç²¾ç¡®ä½¿ç”¨é‡")
                        llm_end_content = self._serialize_model_response(
                            final_llm_response
                        )
                    else:
                        logger.warning(
                            "ğŸ’° æ²¡æœ‰LLMå“åº”ä½¿ç”¨é‡ - ä¸ºè®¡è´¹ä¼°ç®—tokenä½¿ç”¨é‡"
                        )
                        llm_end_content = {"model": llm_model, "usage": {}}

                    llm_end_content["streaming"] = True
                    llm_end_content["llm_response_id"] = llm_response_id

                    response_ms = None
                    if first_chunk_time and last_chunk_time:
                        response_ms = int((last_chunk_time - first_chunk_time) * 1000)
                        llm_end_content["response_ms"] = response_ms

                    llm_end_content["choices"] = [
                        {
                            "finish_reason": finish_reason or "interrupted",
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": accumulated_content,
                                "tool_calls": complete_native_tool_calls or None,
                            },
                        }
                    ]

                    usage_info = llm_end_content.get("usage", {})
                    is_estimated = usage_info.get("estimated", False)
                    logger.info(
                        f"ğŸ’° è®¡è´¹æ¢å¤ - ä½¿ç”¨é‡ ({'ä¼°ç®—' if is_estimated else 'ç²¾ç¡®'}): {usage_info}"
                    )

                    llm_end_msg_obj = await self.add_message(
                        thread_id=thread_id,
                        type="llm_response_end",
                        content=llm_end_content,
                        is_llm_message=False,
                        metadata={
                            "thread_run_id": thread_run_id,
                            "llm_response_id": llm_response_id,
                        },
                    )
                    llm_response_end_saved = True
                    # ä¸è¦åœ¨finallyå—ä¸­ç”Ÿæˆ - æµå¯èƒ½å·²å…³é—­ï¼ˆGeneratorExitï¼‰
                    # å‰ç«¯å·²åœæ­¢æ¶ˆè´¹ï¼Œç”Ÿæˆæ²¡æœ‰æ„ä¹‰
                    logger.info(
                        f"âœ… è®¡è´¹æˆåŠŸï¼šåœ¨finallyä¸­ä¸ºç¬¬ #{auto_continue_count + 1} æ¬¡è°ƒç”¨ä¿å­˜llm_response_endï¼ˆ{'ä¼°ç®—' if is_estimated else 'ç²¾ç¡®'}ä½¿ç”¨é‡ï¼‰"
                    )

                except Exception as billing_e:
                    logger.error(
                        f"âŒ å…³é”®è®¡è´¹å¤±è´¥ï¼šæ— æ³•ä¿å­˜llm_response_end: {str(billing_e)}",
                        exc_info=True,
                    )

            elif llm_response_end_saved:
                logger.debug(
                    f"âœ… ç¬¬ #{auto_continue_count + 1} æ¬¡è°ƒç”¨çš„è®¡è´¹å·²å¤„ç†ï¼ˆllm_response_endå·²æå‰ä¿å­˜ï¼‰"
                )

            if should_auto_continue:
                continuous_state["accumulated_content"] = accumulated_content
                continuous_state["sequence"] = __sequence

                logger.debug(
                    f"ä½¿ç”¨ {len(accumulated_content)} ä¸ªå­—ç¬¦æ›´æ–°è‡ªåŠ¨ç»§ç»­çš„æŒç»­çŠ¶æ€"
                )
            else:
                # ä¿å­˜å¹¶ç”Ÿæˆæœ€ç»ˆçš„thread_run_endçŠ¶æ€ï¼ˆä»…åœ¨éè‡ªåŠ¨ç»§ç»­ä¸”finish_reasonä¸æ˜¯'length'æ—¶ï¼‰
                try:
                    # åœ¨å…ƒæ•°æ®ä¸­å­˜å‚¨last_usageä»¥ç”¨äºå¿«é€Ÿè·¯å¾„ä¼˜åŒ–
                    usage = (
                        final_llm_response.usage
                        if "final_llm_response" in locals()
                        and final_llm_response is not None
                        and hasattr(final_llm_response, "usage")
                        else None
                    )

                    # å¦‚æœæ²¡æœ‰ç²¾ç¡®ä½¿ç”¨é‡ï¼ˆæµæå‰åœæ­¢ï¼‰ï¼Œä½¿ç”¨åœ¨thread_managerä¸­é¢„å…ˆè®¡ç®—çš„estimated_total
                    if not usage and estimated_total_tokens:
                        # é‡ç”¨æˆ‘ä»¬å·²åœ¨thread_managerä¸­è®¡ç®—çš„estimated_totalï¼ˆæ— éœ€æ•°æ®åº“è°ƒç”¨ï¼ï¼‰
                        class EstimatedUsage:
                            def __init__(self, total):
                                self.total_tokens = total

                        usage = EstimatedUsage(estimated_total_tokens)
                        logger.info(
                            f"âš¡ ä½¿ç”¨å¿«é€Ÿæ£€æŸ¥ä¼°ç®—: {estimated_total_tokens} ä¸ªtokenï¼ˆæµå·²åœæ­¢ï¼Œæ— éœ€é‡æ–°è®¡ç®—ï¼‰"
                        )

                    end_content = {"status_type": "thread_run_end"}

                    end_msg_obj = await self.add_message(
                        thread_id=thread_id,
                        type="status",
                        content=end_content,
                        is_llm_message=False,
                        metadata={
                            "thread_run_id": thread_run_id
                            if "thread_run_id" in locals()
                            else None
                        },
                    )
                    # ä¸è¦åœ¨finallyå—ä¸­ç”Ÿæˆ - æµå¯èƒ½å·²å…³é—­ï¼ˆGeneratorExitï¼‰
                    logger.debug(
                        "åœ¨finallyä¸­ä¿å­˜thread_run_endï¼ˆä¸ç”Ÿæˆä»¥é¿å…GeneratorExitï¼‰"
                    )
                except Exception as final_e:
                    logger.error(
                        f"finallyå—ä¸­å‡ºé”™: {str(final_e)}", exc_info=True
                    )

    async def process_non_streaming_response(
        self,
        llm_response: Any,
        thread_id: str,
        prompt_messages: List[Dict[str, Any]],
        llm_model: str,
        config: ProcessorConfig = ProcessorConfig(),
        estimated_total_tokens: Optional[int] = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """å¤„ç†éæµå¼LLMå“åº”ï¼Œå¤„ç†å·¥å…·è°ƒç”¨å’Œæ‰§è¡Œã€‚

        å‚æ•°:
            llm_response: LLMçš„å“åº”
            thread_id: å¯¹è¯çº¿ç¨‹çš„ID
            prompt_messages: å‘é€ç»™LLMçš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæç¤ºï¼‰
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
            config: è§£æå’Œæ‰§è¡Œçš„é…ç½®

        ç”Ÿæˆ:
            åŒ¹é…æ•°æ®åº“æ¨¡å¼çš„å®Œæ•´æ¶ˆæ¯å¯¹è±¡ã€‚
        """
        content = ""
        thread_run_id = str(uuid.uuid4())
        assistant_message_object = None
        finish_reason = None
        native_tool_calls_for_message = []

        try:
            # ä¿å­˜å¹¶ç”Ÿæˆthread_run_startçŠ¶æ€æ¶ˆæ¯
            start_content = {
                "status_type": "thread_run_start",
                "thread_run_id": thread_run_id,
            }
            start_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=start_content,
                is_llm_message=False,
                metadata={"thread_run_id": thread_run_id},
            )
            if start_msg_obj:
                yield format_for_yield(start_msg_obj)

            # æå–finish_reasonã€å†…å®¹ã€å·¥å…·è°ƒç”¨
            if hasattr(llm_response, "choices") and llm_response.choices:
                if hasattr(llm_response.choices[0], "finish_reason"):
                    finish_reason = llm_response.choices[0].finish_reason
                    logger.debug(f"éæµå¼finish_reason: {finish_reason}")
                response_message = (
                    llm_response.choices[0].message
                    if hasattr(llm_response.choices[0], "message")
                    else None
                )
                if response_message:
                    if (
                        hasattr(response_message, "content")
                        and response_message.content
                    ):
                        content = response_message.content

            # --- ä¿å­˜å¹¶ç”Ÿæˆæœ€ç»ˆåŠ©æ‰‹æ¶ˆæ¯ ---
            message_data = {
                "role": "assistant",
                "content": content,
                "tool_calls": native_tool_calls_for_message or None,
            }
            assistant_message_object = await self._add_message_with_agent_info(
                thread_id=thread_id,
                type="assistant",
                content=message_data,
                is_llm_message=True,
                metadata={"thread_run_id": thread_run_id},
            )
            if assistant_message_object:
                yield assistant_message_object
            else:
                logger.error(
                    f"ä¸ºçº¿ç¨‹ {thread_id} ä¿å­˜éæµå¼åŠ©æ‰‹æ¶ˆæ¯å¤±è´¥"
                )
                err_content = {
                    "role": "system",
                    "status_type": "error",
                    "message": "ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯å¤±è´¥",
                }
                err_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=err_content,
                    is_llm_message=False,
                    metadata={"thread_run_id": thread_run_id},
                )
                if err_msg_obj:
                    yield format_for_yield(err_msg_obj)

            # --- ä¿å­˜å¹¶ç”Ÿæˆæœ€ç»ˆçŠ¶æ€ ---
            if finish_reason:
                finish_content = {
                    "status_type": "finish",
                    "finish_reason": finish_reason,
                }
                finish_msg_obj = await self.add_message(
                    thread_id=thread_id,
                    type="status",
                    content=finish_content,
                    is_llm_message=False,
                    metadata={"thread_run_id": thread_run_id},
                )
                if finish_msg_obj:
                    yield format_for_yield(finish_msg_obj)

            # --- ä¿å­˜å¹¶ç”Ÿæˆassistant_response_end ---
            if assistant_message_object:  # ä»…åœ¨ä¿å­˜äº†åŠ©æ‰‹æ¶ˆæ¯æ—¶ä¿å­˜
                try:
                    # å°†LiteLLM ModelResponseè½¬æ¢ä¸ºå¯JSONåºåˆ—åŒ–çš„å­—å…¸
                    response_dict = self._serialize_model_response(llm_response)

                    # åœ¨å†…å®¹ä¸­ä¿å­˜åºåˆ—åŒ–çš„å“åº”å¯¹è±¡
                    await self.add_message(
                        thread_id=thread_id,
                        type="assistant_response_end",
                        content=response_dict,
                        is_llm_message=False,
                        metadata={"thread_run_id": thread_run_id},
                    )
                    logger.debug("éæµå¼å“åº”çš„åŠ©æ‰‹å“åº”ç»“æŸå·²ä¿å­˜")
                except Exception as e:
                    logger.error(
                        f"ä¸ºéæµå¼ä¿å­˜åŠ©æ‰‹å“åº”ç»“æŸæ—¶å‡ºé”™: {str(e)}"
                    )

        except Exception as e:
            # ä½¿ç”¨ErrorProcessorè¿›è¡Œä¸€è‡´çš„é”™è¯¯å¤„ç†
            processed_error = ErrorProcessor.process_system_error(
                e, context={"thread_id": thread_id}
            )
            ErrorProcessor.log_error(processed_error)

            # ä¿å­˜å¹¶ç”Ÿæˆé”™è¯¯çŠ¶æ€
            err_content = {
                "role": "system",
                "status_type": "error",
                "message": processed_error.message,
            }
            err_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=err_content,
                is_llm_message=False,
                metadata={
                    "thread_run_id": thread_run_id
                    if "thread_run_id" in locals()
                    else None
                },
            )
            if err_msg_obj:
                yield format_for_yield(err_msg_obj)

            raise

        finally:
            # ä¿å­˜å¹¶ç”Ÿæˆæœ€ç»ˆçš„thread_run_endçŠ¶æ€
            usage = llm_response.usage if hasattr(llm_response, "usage") else None

            end_content = {"status_type": "thread_run_end"}

            end_msg_obj = await self.add_message(
                thread_id=thread_id,
                type="status",
                content=end_content,
                is_llm_message=False,
                metadata={
                    "thread_run_id": thread_run_id
                    if "thread_run_id" in locals()
                    else None
                },
            )
            if end_msg_obj:
                yield format_for_yield(end_msg_obj)

    async def _add_tool_result(
        self,
        thread_id: str,
        tool_call: Dict[str, Any],
        result: ToolResult,
        strategy: Union[XmlAddingStrategy, str] = "assistant_message",
        assistant_message_id: Optional[str] = None,
        parsing_details: Optional[Dict[str, Any]] = None,
    ) -> Optional[Dict[str, Any]]:  # è¿”å›å®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡
        """æ ¹æ®æŒ‡å®šæ ¼å¼å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯çº¿ç¨‹ã€‚

        è¯¥æ–¹æ³•æ ¼å¼åŒ–å·¥å…·ç»“æœå¹¶å°†å…¶æ·»åŠ åˆ°å¯¹è¯å†å²è®°å½•ä¸­ï¼Œ
        ä½¿å…¶åœ¨åç»­äº¤äº’ä¸­å¯¹LLMå¯è§ã€‚ç»“æœå¯ä»¥ä½œä¸º
        åŸç”Ÿå·¥å…·æ¶ˆæ¯ï¼ˆOpenAIæ ¼å¼ï¼‰æˆ–å¸¦æœ‰æŒ‡å®šè§’è‰²ï¼ˆç”¨æˆ·æˆ–åŠ©æ‰‹ï¼‰çš„XMLåŒ…è£…å†…å®¹æ·»åŠ ã€‚

        å‚æ•°:
            thread_id: å¯¹è¯çº¿ç¨‹çš„ID
            tool_call: äº§ç”Ÿæ­¤ç»“æœçš„åŸå§‹å·¥å…·è°ƒç”¨
            result: å·¥å…·æ‰§è¡Œçš„ç»“æœ
            strategy: å¦‚ä½•å°†XMLå·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯
                     ("user_message", "assistant_message", æˆ– "inline_edit")
            assistant_message_id: ç”Ÿæˆæ­¤å·¥å…·è°ƒç”¨çš„åŠ©æ‰‹æ¶ˆæ¯ID
            parsing_details: XMLè°ƒç”¨çš„å¯é€‰è§£æè¯¦æƒ…ï¼ˆå±æ€§ã€å…ƒç´ ç­‰ï¼‰
        """
        try:
            message_obj = None  # åˆå§‹åŒ–message_obj

            # å¦‚æœæä¾›äº†assistant_message_idï¼Œåˆ™åˆ›å»ºåŒ…å«å®ƒçš„å…ƒæ•°æ®
            metadata = {}
            if assistant_message_id:
                metadata["assistant_message_id"] = assistant_message_id
                logger.debug(
                    f"å°†å·¥å…·ç»“æœé“¾æ¥åˆ°åŠ©æ‰‹æ¶ˆæ¯: {assistant_message_id}"
                )

            # --- å¦‚æœå¯ç”¨ï¼Œå°†è§£æè¯¦æƒ…æ·»åŠ åˆ°å…ƒæ•°æ® ---
            if parsing_details:
                metadata["parsing_details"] = parsing_details
                logger.debug("å°†parsing_detailsæ·»åŠ åˆ°å·¥å…·ç»“æœå…ƒæ•°æ®")
            # ---

            # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯åŸç”Ÿå‡½æ•°è°ƒç”¨ï¼ˆå…·æœ‰idå­—æ®µï¼‰
            if "id" in tool_call:
                # æ ¹æ®OpenAIè§„èŒƒæ ¼å¼åŒ–ä¸ºé€‚å½“çš„å·¥å…·æ¶ˆæ¯
                function_name = tool_call.get("function_name", "")

                # æ ¼å¼åŒ–å·¥å…·ç»“æœå†…å®¹ - å·¥å…·è§’è‰²éœ€è¦å­—ç¬¦ä¸²å†…å®¹
                if isinstance(result, str):
                    content = result
                elif hasattr(result, "output"):
                    # å¦‚æœæ˜¯ToolResultå¯¹è±¡
                    if isinstance(result.output, dict) or isinstance(
                        result.output, list
                    ):
                        # å¦‚æœè¾“å‡ºå·²ç»æ˜¯dictæˆ–listï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                        content = json.dumps(result.output)
                    else:
                        # å¦åˆ™ä»…ä½¿ç”¨å­—ç¬¦ä¸²è¡¨ç¤º
                        content = str(result.output)
                else:
                    # å›é€€åˆ°æ•´ä¸ªç»“æœçš„å­—ç¬¦ä¸²è¡¨ç¤º
                    content = str(result)

                logger.debug(f"æ ¼å¼åŒ–çš„å·¥å…·ç»“æœå†…å®¹: {content[:100]}...")

                # åˆ›å»ºå…·æœ‰é€‚å½“æ ¼å¼çš„å·¥å…·å“åº”æ¶ˆæ¯
                tool_message = {
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "name": function_name,
                    "content": content,
                }

                logger.debug(
                    f"ä¸ºtool_call_id={tool_call['id']}æ·»åŠ åŸç”Ÿå·¥å…·ç»“æœï¼Œè§’è‰²ä¸ºtool"
                )

                # ä½œä¸ºå·¥å…·æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²è®°å½•
                # è¿™ä½¿ç»“æœåœ¨ä¸‹ä¸€è½®å¯¹LLMå¯è§
                message_obj = await self.add_message(
                    thread_id=thread_id,
                    type="tool",  # å·¥å…·å“åº”çš„ç‰¹æ®Šç±»å‹
                    content=tool_message,
                    is_llm_message=True,
                    metadata=metadata,
                )
                return message_obj  # è¿”å›å®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡

            # å¯¹äºXMLå’Œå…¶ä»–éåŸç”Ÿå·¥å…·ï¼Œä½¿ç”¨æ–°çš„ç»“æ„åŒ–æ ¼å¼
            # æ ¹æ®ç­–ç•¥ç¡®å®šæ¶ˆæ¯è§’è‰²
            result_role = "user" if strategy == "user_message" else "assistant"

            # åˆ›å»ºä¸¤ä¸ªç‰ˆæœ¬çš„ç»“æ„åŒ–ç»“æœ
            # 1. ç”¨äºå‰ç«¯çš„å¯Œç‰ˆæœ¬
            structured_result_for_frontend = self._create_structured_tool_result(
                tool_call, result, parsing_details, for_llm=False
            )
            # 2. ç”¨äºLLMçš„ç®€æ´ç‰ˆæœ¬
            structured_result_for_llm = self._create_structured_tool_result(
                tool_call, result, parsing_details, for_llm=True
            )

            # å°†å…·æœ‰é€‚å½“è§’è‰²çš„æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²è®°å½•
            # è¿™å…è®¸LLMåœ¨åç»­äº¤äº’ä¸­çœ‹åˆ°å·¥å…·ç»“æœ
            result_message_for_llm = {
                "role": result_role,
                "content": json.dumps(structured_result_for_llm),
            }

            # å°†å¯Œå†…å®¹æ·»åŠ åˆ°å…ƒæ•°æ®ä»¥ä¾›å‰ç«¯ä½¿ç”¨
            if metadata is None:
                metadata = {}
            metadata["frontend_content"] = structured_result_for_frontend

            message_obj = await self._add_message_with_agent_info(
                thread_id=thread_id,
                type="tool",
                content=result_message_for_llm,  # ä¿å­˜LLMå‹å¥½ç‰ˆæœ¬
                is_llm_message=True,
                metadata=metadata,
            )

            # å¦‚æœæ¶ˆæ¯å·²ä¿å­˜ï¼Œåœ¨è¿”å›å‰åœ¨å†…å­˜ä¸­ä¸ºå‰ç«¯ä¿®æ”¹å®ƒ
            if message_obj:
                # å‰ç«¯æœŸæœ›åœ¨'content'å­—æ®µä¸­æœ‰å¯Œå†…å®¹ã€‚
                # æ•°æ®åº“åœ¨metadata.frontend_contentä¸­æœ‰å¯Œå†…å®¹ã€‚
                # è®©æˆ‘ä»¬é‡æ„æ¶ˆæ¯ä»¥ä¾›ç”Ÿæˆã€‚
                message_for_yield = message_obj.copy()
                message_for_yield["content"] = structured_result_for_frontend
                return message_for_yield

            return message_obj  # è¿”å›ä¿®æ”¹åçš„æ¶ˆæ¯å¯¹è±¡
        except Exception as e:
            logger.error(f"æ·»åŠ å·¥å…·ç»“æœæ—¶å‡ºé”™: {str(e)}", exc_info=True)
            # å›é€€åˆ°ç®€å•æ¶ˆæ¯
            try:
                fallback_message = {"role": "user", "content": str(result)}
                message_obj = await self.add_message(
                    thread_id=thread_id,
                    type="tool",
                    content=fallback_message,
                    is_llm_message=True,
                    metadata={"assistant_message_id": assistant_message_id}
                    if assistant_message_id
                    else {},
                )
                return message_obj  # è¿”å›å®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡
            except Exception as e2:
                logger.error(
                    f"å³ä½¿ä½¿ç”¨å›é€€æ¶ˆæ¯ä¹Ÿå¤±è´¥: {str(e2)}", exc_info=True
                )
                return None  # å‡ºé”™æ—¶è¿”å›None

    def _create_structured_tool_result(
        self,
        tool_call: Dict[str, Any],
        result: ToolResult,
        parsing_details: Optional[Dict[str, Any]] = None,
        for_llm: bool = False,
    ):
        """åˆ›å»ºä¸å·¥å…·æ— å…³ä¸”æä¾›ä¸°å¯Œä¿¡æ¯çš„ç»“æ„åŒ–å·¥å…·ç»“æœæ ¼å¼ã€‚

        å‚æ•°:
            tool_call: è¢«æ‰§è¡Œçš„åŸå§‹å·¥å…·è°ƒç”¨
            result: å·¥å…·æ‰§è¡Œçš„ç»“æœ
            parsing_details: XMLè°ƒç”¨çš„å¯é€‰è§£æè¯¦æƒ…
            for_llm: å¦‚æœä¸ºTrueï¼Œä¸ºLLMä¸Šä¸‹æ–‡åˆ›å»ºç®€æ´ç‰ˆæœ¬ã€‚

        è¿”å›:
            åŒ…å«å·¥å…·æ‰§è¡Œä¿¡æ¯çš„ç»“æ„åŒ–å­—å…¸
        """
        # æå–å·¥å…·ä¿¡æ¯
        function_name = tool_call.get("function_name", "unknown")
        xml_tag_name = tool_call.get("xml_tag_name")
        arguments = tool_call.get("arguments", {})
        tool_call_id = tool_call.get("id")

        # å¤„ç†è¾“å‡º - å¦‚æœæ˜¯JSONå­—ç¬¦ä¸²ï¼Œåˆ™è§£æå›å¯¹è±¡
        output = result.output if hasattr(result, "output") else str(result)
        if isinstance(output, str):
            try:
                # å°è¯•è§£æä¸ºJSONä»¥å‘å‰ç«¯æä¾›ç»“æ„åŒ–æ•°æ®
                parsed_output = safe_json_parse(output)
                # å¦‚æœè§£ææˆåŠŸä¸”å¾—åˆ°dict/listï¼Œåˆ™ä½¿ç”¨è§£æç‰ˆæœ¬
                if isinstance(parsed_output, (dict, list)):
                    output = parsed_output
                # å¦åˆ™ä¿ç•™åŸå§‹å­—ç¬¦ä¸²
            except Exception:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä¿ç•™åŸå§‹å­—ç¬¦ä¸²
                pass

        structured_result_v1 = {
            "tool_execution": {
                "function_name": function_name,
                "xml_tag_name": xml_tag_name,
                "tool_call_id": tool_call_id,
                "arguments": arguments,
                "result": {
                    "success": result.success if hasattr(result, "success") else True,
                    "output": output,
                    "error": getattr(result, "error", None)
                    if hasattr(result, "error")
                    else None,
                },
            }
        }

        return structured_result_v1

    def _create_tool_context(
        self,
        tool_call: Dict[str, Any],
        tool_index: int,
        assistant_message_id: Optional[str] = None,
        parsing_details: Optional[Dict[str, Any]] = None,
    ) -> ToolExecutionContext:
        """åˆ›å»ºåŒ…å«æ˜¾ç¤ºåç§°å’Œè§£æè¯¦æƒ…çš„å·¥å…·æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚"""
        context = ToolExecutionContext(
            tool_call=tool_call,
            tool_index=tool_index,
            assistant_message_id=assistant_message_id,
            parsing_details=parsing_details,
        )

        # è®¾ç½®function_nameå’Œxml_tag_nameå­—æ®µ
        if "xml_tag_name" in tool_call:
            context.xml_tag_name = tool_call["xml_tag_name"]
            context.function_name = tool_call.get(
                "function_name", tool_call["xml_tag_name"]
            )
        else:
            # å¯¹äºéXMLå·¥å…·ï¼Œç›´æ¥ä½¿ç”¨å‡½æ•°å
            context.function_name = tool_call.get("function_name", "unknown")
            context.xml_tag_name = None

        return context

    async def _yield_and_save_tool_started(
        self, context: ToolExecutionContext, thread_id: str, thread_run_id: str
    ) -> Optional[Dict[str, Any]]:
        """æ ¼å¼åŒ–ã€ä¿å­˜å¹¶è¿”å›å·¥å…·å¼€å§‹çŠ¶æ€æ¶ˆæ¯ã€‚"""
        tool_name = context.xml_tag_name or context.function_name
        content = {
            "role": "assistant",
            "status_type": "tool_started",
            "function_name": context.function_name,
            "xml_tag_name": context.xml_tag_name,
            "message": f"å¼€å§‹æ‰§è¡Œ {tool_name}",
            "tool_index": context.tool_index,
            "tool_call_id": context.tool_call.get(
                "id"
            ),  # å¦‚æœæ˜¯åŸç”Ÿçš„ï¼ŒåŒ…å«tool_call ID
        }
        metadata = {"thread_run_id": thread_run_id}
        saved_message_obj = await self.add_message(
            thread_id=thread_id,
            type="status",
            content=content,
            is_llm_message=False,
            metadata=metadata,
        )
        return saved_message_obj  # è¿”å›å®Œæ•´å¯¹è±¡ï¼ˆå¦‚æœä¿å­˜å¤±è´¥åˆ™è¿”å›Noneï¼‰

    async def _yield_and_save_tool_completed(
        self,
        context: ToolExecutionContext,
        tool_message_id: Optional[str],
        thread_id: str,
        thread_run_id: str,
    ) -> Optional[Dict[str, Any]]:
        """æ ¼å¼åŒ–ã€ä¿å­˜å¹¶è¿”å›å·¥å…·å®Œæˆ/å¤±è´¥çŠ¶æ€æ¶ˆæ¯ã€‚"""
        if not context.result:
            # å¦‚æœç»“æœç¼ºå¤±ï¼ˆä¾‹å¦‚æ‰§è¡Œå¤±è´¥ï¼‰ï¼Œå§”æ‰˜ç»™é”™è¯¯ä¿å­˜
            return await self._yield_and_save_tool_error(
                context, thread_id, thread_run_id
            )

        tool_name = context.xml_tag_name or context.function_name
        status_type = "tool_completed" if context.result.success else "tool_failed"
        message_text = f"å·¥å…· {tool_name} {'æˆåŠŸå®Œæˆ' if context.result.success else 'å¤±è´¥'}"

        content = {
            "role": "assistant",
            "status_type": status_type,
            "function_name": context.function_name,
            "xml_tag_name": context.xml_tag_name,
            "message": message_text,
            "tool_index": context.tool_index,
            "tool_call_id": context.tool_call.get("id"),
        }
        metadata = {"thread_run_id": thread_run_id}
        # å¦‚æœå¯ç”¨ä¸”æˆåŠŸï¼Œå°†*å®é™…*å·¥å…·ç»“æœæ¶ˆæ¯IDæ·»åŠ åˆ°å…ƒæ•°æ®
        if context.result.success and tool_message_id:
            metadata["linked_tool_result_message_id"] = tool_message_id

        # <<< æ·»åŠ ï¼šå¦‚æœè¿™æ˜¯ç»ˆæ­¢å·¥å…·ï¼Œåˆ™å‘å‡ºä¿¡å· >>>
        if context.function_name in ["ask", "complete"]:
            metadata["agent_should_terminate"] = "true"
            logger.debug(
                f"ä½¿ç”¨ç»ˆæ­¢ä¿¡å·æ ‡è®°å·¥å…·çŠ¶æ€ '{context.function_name}'ã€‚"
            )
        # <<< ç»“æŸæ·»åŠ  >>>

        saved_message_obj = await self.add_message(
            thread_id=thread_id,
            type="status",
            content=content,
            is_llm_message=False,
            metadata=metadata,
        )
        return saved_message_obj

    async def _yield_and_save_tool_error(
        self, context: ToolExecutionContext, thread_id: str, thread_run_id: str
    ) -> Optional[Dict[str, Any]]:
        """æ ¼å¼åŒ–ã€ä¿å­˜å¹¶è¿”å›å·¥å…·é”™è¯¯çŠ¶æ€æ¶ˆæ¯ã€‚"""
        error_msg = (
            str(context.error)
            if context.error
            else "å·¥å…·æ‰§è¡ŒæœŸé—´æœªçŸ¥é”™è¯¯"
        )
        tool_name = context.xml_tag_name or context.function_name
        content = {
            "role": "assistant",
            "status_type": "tool_error",
            "function_name": context.function_name,
            "xml_tag_name": context.xml_tag_name,
            "message": f"æ‰§è¡Œå·¥å…· {tool_name} æ—¶å‡ºé”™: {error_msg}",
            "tool_index": context.tool_index,
            "tool_call_id": context.tool_call.get("id"),
        }
        metadata = {"thread_run_id": thread_run_id}
        # ä½¿ç”¨is_llm_message=Falseä¿å­˜çŠ¶æ€æ¶ˆæ¯
        saved_message_obj = await self.add_message(
            thread_id=thread_id,
            type="status",
            content=content,
            is_llm_message=False,
            metadata=metadata,
        )
        return saved_message_obj
